{
  "provider": "huggingface_inference",
  "model_count": 500,
  "models": [
    {
      "id": "huggingface_inference/Lightricks/LTX-2",
      "name": "Lightricks/LTX-2",
      "provider": "huggingface_inference",
      "model_id": "Lightricks/LTX-2"
    },
    {
      "id": "huggingface_inference/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA",
      "name": "fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA",
      "provider": "huggingface_inference",
      "model_id": "fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen-Image-2512",
      "name": "Qwen/Qwen-Image-2512",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen-Image-2512"
    },
    {
      "id": "huggingface_inference/MiniMaxAI/MiniMax-M2.1",
      "name": "MiniMaxAI/MiniMax-M2.1",
      "provider": "huggingface_inference",
      "model_id": "MiniMaxAI/MiniMax-M2.1"
    },
    {
      "id": "huggingface_inference/zai-org/GLM-4.7",
      "name": "zai-org/GLM-4.7",
      "provider": "huggingface_inference",
      "model_id": "zai-org/GLM-4.7",
      "benchmarks": {
        "simplebench": 0.477,
        "livebench_global": 60.73,
        "livebench_reasoning": 59.73,
        "livebench_coding": 73.13,
        "livebench_agentic_coding": 41.67,
        "livebench_math": 76.02,
        "livebench_data_analysis": 73.68,
        "livebench_language": 65.23,
        "livebench_ifeval": 35.66
      }
    },
    {
      "id": "huggingface_inference/Tongyi-MAI/Z-Image-Turbo",
      "name": "Tongyi-MAI/Z-Image-Turbo",
      "provider": "huggingface_inference",
      "model_id": "Tongyi-MAI/Z-Image-Turbo"
    },
    {
      "id": "huggingface_inference/lilylilith/AnyPose",
      "name": "lilylilith/AnyPose",
      "provider": "huggingface_inference",
      "model_id": "lilylilith/AnyPose"
    },
    {
      "id": "huggingface_inference/fal/FLUX.2-dev-Turbo",
      "name": "fal/FLUX.2-dev-Turbo",
      "provider": "huggingface_inference",
      "model_id": "fal/FLUX.2-dev-Turbo"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen-Image-Edit-2511",
      "name": "Qwen/Qwen-Image-Edit-2511",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen-Image-Edit-2511"
    },
    {
      "id": "huggingface_inference/valiantcat/Qwen-Image-Edit-2511-Upscale2K",
      "name": "valiantcat/Qwen-Image-Edit-2511-Upscale2K",
      "provider": "huggingface_inference",
      "model_id": "valiantcat/Qwen-Image-Edit-2511-Upscale2K"
    },
    {
      "id": "huggingface_inference/lrzjason/Anything2Real_2601",
      "name": "lrzjason/Anything2Real_2601",
      "provider": "huggingface_inference",
      "model_id": "lrzjason/Anything2Real_2601"
    },
    {
      "id": "huggingface_inference/openai/whisper-large-v3",
      "name": "openai/whisper-large-v3",
      "provider": "huggingface_inference",
      "model_id": "openai/whisper-large-v3"
    },
    {
      "id": "huggingface_inference/meta-llama/Llama-3.1-8B-Instruct",
      "name": "meta-llama/Llama-3.1-8B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Llama-3.1-8B-Instruct",
      "benchmarks": {
        "gpqa_diamond": 0.259,
        "mmlu": 0.561,
        "math_level_5": 0.229,
        "gsm8k": 0.824,
        "otis_mock_aime_2024_2025": 0.025
      }
    },
    {
      "id": "huggingface_inference/hexgrad/Kokoro-82M",
      "name": "hexgrad/Kokoro-82M",
      "provider": "huggingface_inference",
      "model_id": "hexgrad/Kokoro-82M"
    },
    {
      "id": "huggingface_inference/black-forest-labs/FLUX.2-dev",
      "name": "black-forest-labs/FLUX.2-dev",
      "provider": "huggingface_inference",
      "model_id": "black-forest-labs/FLUX.2-dev"
    },
    {
      "id": "huggingface_inference/lightx2v/Qwen-Image-Edit-2511-Lightning",
      "name": "lightx2v/Qwen-Image-Edit-2511-Lightning",
      "provider": "huggingface_inference",
      "model_id": "lightx2v/Qwen-Image-Edit-2511-Lightning"
    },
    {
      "id": "huggingface_inference/openai/gpt-oss-20b",
      "name": "openai/gpt-oss-20b",
      "provider": "huggingface_inference",
      "model_id": "openai/gpt-oss-20b",
      "benchmarks": {
        "arena_elo": 1318.0
      }
    },
    {
      "id": "huggingface_inference/black-forest-labs/FLUX.1-dev",
      "name": "black-forest-labs/FLUX.1-dev",
      "provider": "huggingface_inference",
      "model_id": "black-forest-labs/FLUX.1-dev"
    },
    {
      "id": "huggingface_inference/prithivMLmods/Qwen-Image-Edit-2511-Object-Remover",
      "name": "prithivMLmods/Qwen-Image-Edit-2511-Object-Remover",
      "provider": "huggingface_inference",
      "model_id": "prithivMLmods/Qwen-Image-Edit-2511-Object-Remover"
    },
    {
      "id": "huggingface_inference/prithivMLmods/Qwen-Image-Edit-2511-Anime",
      "name": "prithivMLmods/Qwen-Image-Edit-2511-Anime",
      "provider": "huggingface_inference",
      "model_id": "prithivMLmods/Qwen-Image-Edit-2511-Anime"
    },
    {
      "id": "huggingface_inference/deepseek-ai/DeepSeek-V3.2",
      "name": "deepseek-ai/DeepSeek-V3.2",
      "provider": "huggingface_inference",
      "model_id": "deepseek-ai/DeepSeek-V3.2",
      "benchmarks": {
        "aider_polyglot": 70.2,
        "arena_elo": 1423.0,
        "livebench_global": 52.82,
        "livebench_reasoning": 45.5,
        "livebench_coding": 73.19,
        "livebench_agentic_coding": 36.67,
        "livebench_math": 64.38,
        "livebench_data_analysis": 65.09,
        "livebench_language": 65.6,
        "livebench_ifeval": 19.33
      }
    },
    {
      "id": "huggingface_inference/XiaomiMiMo/MiMo-V2-Flash",
      "name": "XiaomiMiMo/MiMo-V2-Flash",
      "provider": "huggingface_inference",
      "model_id": "XiaomiMiMo/MiMo-V2-Flash"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-VL-8B-Instruct",
      "name": "Qwen/Qwen3-VL-8B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-VL-8B-Instruct"
    },
    {
      "id": "huggingface_inference/stabilityai/stable-diffusion-xl-base-1.0",
      "name": "stabilityai/stable-diffusion-xl-base-1.0",
      "provider": "huggingface_inference",
      "model_id": "stabilityai/stable-diffusion-xl-base-1.0"
    },
    {
      "id": "huggingface_inference/deepseek-ai/DeepSeek-OCR",
      "name": "deepseek-ai/DeepSeek-OCR",
      "provider": "huggingface_inference",
      "model_id": "deepseek-ai/DeepSeek-OCR"
    },
    {
      "id": "huggingface_inference/sentence-transformers/all-MiniLM-L6-v2",
      "name": "sentence-transformers/all-MiniLM-L6-v2",
      "provider": "huggingface_inference",
      "model_id": "sentence-transformers/all-MiniLM-L6-v2"
    },
    {
      "id": "huggingface_inference/black-forest-labs/FLUX.1-schnell",
      "name": "black-forest-labs/FLUX.1-schnell",
      "provider": "huggingface_inference",
      "model_id": "black-forest-labs/FLUX.1-schnell"
    },
    {
      "id": "huggingface_inference/meta-llama/Llama-3.2-3B-Instruct",
      "name": "meta-llama/Llama-3.2-3B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Llama-3.2-3B-Instruct"
    },
    {
      "id": "huggingface_inference/briaai/RMBG-2.0",
      "name": "briaai/RMBG-2.0",
      "provider": "huggingface_inference",
      "model_id": "briaai/RMBG-2.0"
    },
    {
      "id": "huggingface_inference/ResembleAI/chatterbox",
      "name": "ResembleAI/chatterbox",
      "provider": "huggingface_inference",
      "model_id": "ResembleAI/chatterbox"
    },
    {
      "id": "huggingface_inference/BAAI/bge-m3",
      "name": "BAAI/bge-m3",
      "provider": "huggingface_inference",
      "model_id": "BAAI/bge-m3"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen-Image",
      "name": "Qwen/Qwen-Image",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen-Image"
    },
    {
      "id": "huggingface_inference/openai/gpt-oss-120b",
      "name": "openai/gpt-oss-120b",
      "provider": "huggingface_inference",
      "model_id": "openai/gpt-oss-120b",
      "benchmarks": {
        "simplebench": 0.221,
        "arena_elo": 1353.0,
        "livebench_global": 48.66,
        "livebench_reasoning": 39.21,
        "livebench_coding": 60.21,
        "livebench_agentic_coding": 16.67,
        "livebench_math": 68.87,
        "livebench_data_analysis": 56.77,
        "livebench_language": 48.59,
        "livebench_ifeval": 50.29
      }
    },
    {
      "id": "huggingface_inference/google/gemma-3-12b-it-qat-q4_0-unquantized",
      "name": "google/gemma-3-12b-it-qat-q4_0-unquantized",
      "provider": "huggingface_inference",
      "model_id": "google/gemma-3-12b-it-qat-q4_0-unquantized"
    },
    {
      "id": "huggingface_inference/dphn/Dolphin-Mistral-24B-Venice-Edition",
      "name": "dphn/Dolphin-Mistral-24B-Venice-Edition",
      "provider": "huggingface_inference",
      "model_id": "dphn/Dolphin-Mistral-24B-Venice-Edition"
    },
    {
      "id": "huggingface_inference/google/embeddinggemma-300m",
      "name": "google/embeddinggemma-300m",
      "provider": "huggingface_inference",
      "model_id": "google/embeddinggemma-300m"
    },
    {
      "id": "huggingface_inference/deepseek-ai/DeepSeek-R1",
      "name": "deepseek-ai/DeepSeek-R1",
      "provider": "huggingface_inference",
      "model_id": "deepseek-ai/DeepSeek-R1",
      "benchmarks": {
        "gpqa_diamond": 0.692,
        "math_level_5": 0.931,
        "otis_mock_aime_2024_2025": 0.533,
        "aider_polyglot": 56.9,
        "simplebench": 0.309,
        "simpleqa_verified": 0.274,
        "swe_bench_verified": 0.333,
        "arena_elo": 1396.0
      }
    },
    {
      "id": "huggingface_inference/black-forest-labs/FLUX.1-Kontext-dev",
      "name": "black-forest-labs/FLUX.1-Kontext-dev",
      "provider": "huggingface_inference",
      "model_id": "black-forest-labs/FLUX.1-Kontext-dev"
    },
    {
      "id": "huggingface_inference/Wan-AI/Wan2.2-I2V-A14B",
      "name": "Wan-AI/Wan2.2-I2V-A14B",
      "provider": "huggingface_inference",
      "model_id": "Wan-AI/Wan2.2-I2V-A14B"
    },
    {
      "id": "huggingface_inference/lightx2v/Wan2.2-Distill-Loras",
      "name": "lightx2v/Wan2.2-Distill-Loras",
      "provider": "huggingface_inference",
      "model_id": "lightx2v/Wan2.2-Distill-Loras"
    },
    {
      "id": "huggingface_inference/openai/whisper-large-v3-turbo",
      "name": "openai/whisper-large-v3-turbo",
      "provider": "huggingface_inference",
      "model_id": "openai/whisper-large-v3-turbo"
    },
    {
      "id": "huggingface_inference/stabilityai/stable-diffusion-3.5-large",
      "name": "stabilityai/stable-diffusion-3.5-large",
      "provider": "huggingface_inference",
      "model_id": "stabilityai/stable-diffusion-3.5-large"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-4B-Instruct-2507",
      "name": "Qwen/Qwen3-4B-Instruct-2507",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-4B-Instruct-2507"
    },
    {
      "id": "huggingface_inference/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8",
      "name": "nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8",
      "provider": "huggingface_inference",
      "model_id": "nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8"
    },
    {
      "id": "huggingface_inference/Falconsai/nsfw_image_detection",
      "name": "Falconsai/nsfw_image_detection",
      "provider": "huggingface_inference",
      "model_id": "Falconsai/nsfw_image_detection"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-8B",
      "name": "Qwen/Qwen3-8B",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-8B"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-Coder-30B-A3B-Instruct",
      "name": "Qwen/Qwen3-Coder-30B-A3B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-Coder-30B-A3B-Instruct"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-VL-30B-A3B-Instruct",
      "name": "Qwen/Qwen3-VL-30B-A3B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-VL-30B-A3B-Instruct"
    },
    {
      "id": "huggingface_inference/dx8152/Qwen-Edit-2509-Multiple-angles",
      "name": "dx8152/Qwen-Edit-2509-Multiple-angles",
      "provider": "huggingface_inference",
      "model_id": "dx8152/Qwen-Edit-2509-Multiple-angles"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-Embedding-8B",
      "name": "Qwen/Qwen3-Embedding-8B",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-Embedding-8B"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen-Image-Edit",
      "name": "Qwen/Qwen-Image-Edit",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen-Image-Edit"
    },
    {
      "id": "huggingface_inference/meituan-longcat/LongCat-Video",
      "name": "meituan-longcat/LongCat-Video",
      "provider": "huggingface_inference",
      "model_id": "meituan-longcat/LongCat-Video"
    },
    {
      "id": "huggingface_inference/tencent/HunyuanVideo-1.5",
      "name": "tencent/HunyuanVideo-1.5",
      "provider": "huggingface_inference",
      "model_id": "tencent/HunyuanVideo-1.5"
    },
    {
      "id": "huggingface_inference/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
      "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
      "provider": "huggingface_inference",
      "model_id": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    },
    {
      "id": "huggingface_inference/google/gemma-3-12b-it",
      "name": "google/gemma-3-12b-it",
      "provider": "huggingface_inference",
      "model_id": "google/gemma-3-12b-it",
      "benchmarks": {
        "arena_elo": 1341.0
      }
    },
    {
      "id": "huggingface_inference/meta-llama/Llama-4-Scout-17B-16E-Instruct",
      "name": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
      "benchmarks": {
        "gpqa_diamond": 0.518,
        "math_level_5": 0.623,
        "otis_mock_aime_2024_2025": 0.078,
        "frontiermath": 0.0
      }
    },
    {
      "id": "huggingface_inference/lightx2v/Qwen-Image-Lightning",
      "name": "lightx2v/Qwen-Image-Lightning",
      "provider": "huggingface_inference",
      "model_id": "lightx2v/Qwen-Image-Lightning"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen-Image-Edit-2509",
      "name": "Qwen/Qwen-Image-Edit-2509",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen-Image-Edit-2509"
    },
    {
      "id": "huggingface_inference/moonshotai/Kimi-K2-Thinking",
      "name": "moonshotai/Kimi-K2-Thinking",
      "provider": "huggingface_inference",
      "model_id": "moonshotai/Kimi-K2-Thinking",
      "benchmarks": {
        "frontiermath": 0.214,
        "frontiermath_tier_4": 0.0,
        "humanitys_last_exam": 0.239,
        "livebench_global": 64.2,
        "livebench_reasoning": 63.49,
        "livebench_coding": 67.44,
        "livebench_agentic_coding": 38.33,
        "livebench_math": 81.1,
        "livebench_data_analysis": 70.57,
        "livebench_language": 66.45,
        "livebench_ifeval": 62.03
      }
    },
    {
      "id": "huggingface_inference/Alissonerdx/BFS-Best-Face-Swap",
      "name": "Alissonerdx/BFS-Best-Face-Swap",
      "provider": "huggingface_inference",
      "model_id": "Alissonerdx/BFS-Best-Face-Swap"
    },
    {
      "id": "huggingface_inference/mistralai/Mistral-7B-Instruct-v0.3",
      "name": "mistralai/Mistral-7B-Instruct-v0.3",
      "provider": "huggingface_inference",
      "model_id": "mistralai/Mistral-7B-Instruct-v0.3",
      "benchmarks": {
        "gpqa_diamond": 0.152,
        "mmlu": 0.599,
        "math_level_5": 0.036
      }
    },
    {
      "id": "huggingface_inference/google/gemma-2-2b-it",
      "name": "google/gemma-2-2b-it",
      "provider": "huggingface_inference",
      "model_id": "google/gemma-2-2b-it",
      "benchmarks": {
        "arena_elo": 1199.0
      }
    },
    {
      "id": "huggingface_inference/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
      "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
      "provider": "huggingface_inference",
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    },
    {
      "id": "huggingface_inference/google/gemma-3-27b-it",
      "name": "google/gemma-3-27b-it",
      "provider": "huggingface_inference",
      "model_id": "google/gemma-3-27b-it",
      "benchmarks": {
        "gpqa_diamond": 0.489,
        "math_level_5": 0.74,
        "otis_mock_aime_2024_2025": 0.197,
        "aider_polyglot": 4.9,
        "arena_elo": 1365.0
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-4B-Thinking-2507",
      "name": "Qwen/Qwen3-4B-Thinking-2507",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-4B-Thinking-2507"
    },
    {
      "id": "huggingface_inference/zai-org/GLM-4.6V-Flash",
      "name": "zai-org/GLM-4.6V-Flash",
      "provider": "huggingface_inference",
      "model_id": "zai-org/GLM-4.6V-Flash"
    },
    {
      "id": "huggingface_inference/prithivMLmods/Qwen-Image-Edit-2511-Object-Adder",
      "name": "prithivMLmods/Qwen-Image-Edit-2511-Object-Adder",
      "provider": "huggingface_inference",
      "model_id": "prithivMLmods/Qwen-Image-Edit-2511-Object-Adder"
    },
    {
      "id": "huggingface_inference/meta-llama/Meta-Llama-3-8B-Instruct",
      "name": "meta-llama/Meta-Llama-3-8B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Meta-Llama-3-8B-Instruct",
      "benchmarks": {
        "gpqa_diamond": 0.261,
        "mmlu": 0.665,
        "math_level_5": 0.061,
        "otis_mock_aime_2024_2025": 0.008,
        "trivia_qa": 0.677,
        "arena_elo": 1223.0
      }
    },
    {
      "id": "huggingface_inference/meta-llama/Meta-Llama-3-8B",
      "name": "meta-llama/Meta-Llama-3-8B",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Meta-Llama-3-8B",
      "benchmarks": {
        "gpqa_diamond": 0.261,
        "mmlu": 0.665,
        "math_level_5": 0.061,
        "otis_mock_aime_2024_2025": 0.008,
        "trivia_qa": 0.677,
        "arena_elo": 1223.0
      }
    },
    {
      "id": "huggingface_inference/meta-llama/Llama-3.1-8B",
      "name": "meta-llama/Llama-3.1-8B",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Llama-3.1-8B",
      "benchmarks": {
        "gpqa_diamond": 0.259,
        "mmlu": 0.561,
        "math_level_5": 0.229,
        "gsm8k": 0.824,
        "otis_mock_aime_2024_2025": 0.025
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-Coder-7B-Instruct",
      "name": "Qwen/Qwen2.5-Coder-7B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-Coder-7B-Instruct",
      "benchmarks": {
        "mmlu": 0.68,
        "gsm8k": 0.839
      }
    },
    {
      "id": "huggingface_inference/Lightricks/LTX-Video",
      "name": "Lightricks/LTX-Video",
      "provider": "huggingface_inference",
      "model_id": "Lightricks/LTX-Video"
    },
    {
      "id": "huggingface_inference/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
      "name": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
      "provider": "huggingface_inference",
      "model_id": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"
    },
    {
      "id": "huggingface_inference/HuggingFaceTB/SmolLM3-3B",
      "name": "HuggingFaceTB/SmolLM3-3B",
      "provider": "huggingface_inference",
      "model_id": "HuggingFaceTB/SmolLM3-3B"
    },
    {
      "id": "huggingface_inference/lovis93/next-scene-qwen-image-lora-2509",
      "name": "lovis93/next-scene-qwen-image-lora-2509",
      "provider": "huggingface_inference",
      "model_id": "lovis93/next-scene-qwen-image-lora-2509"
    },
    {
      "id": "huggingface_inference/distilbert/distilbert-base-uncased",
      "name": "distilbert/distilbert-base-uncased",
      "provider": "huggingface_inference",
      "model_id": "distilbert/distilbert-base-uncased"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-7B-Instruct",
      "name": "Qwen/Qwen2.5-7B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-7B-Instruct",
      "benchmarks": {
        "mmlu": 0.729
      }
    },
    {
      "id": "huggingface_inference/moonshotai/Kimi-K2-Instruct",
      "name": "moonshotai/Kimi-K2-Instruct",
      "provider": "huggingface_inference",
      "model_id": "moonshotai/Kimi-K2-Instruct",
      "benchmarks": {
        "aider_polyglot": 59.1,
        "simplebench": 0.263,
        "livebench_global": 50.97,
        "livebench_reasoning": 42.23,
        "livebench_coding": 74.28,
        "livebench_agentic_coding": 31.67,
        "livebench_math": 58.15,
        "livebench_data_analysis": 63.41,
        "livebench_language": 66.69,
        "livebench_ifeval": 20.36
      }
    },
    {
      "id": "huggingface_inference/Wan-AI/Wan2.2-T2V-A14B",
      "name": "Wan-AI/Wan2.2-T2V-A14B",
      "provider": "huggingface_inference",
      "model_id": "Wan-AI/Wan2.2-T2V-A14B"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-30B-A3B-Instruct-2507",
      "name": "Qwen/Qwen3-30B-A3B-Instruct-2507",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-30B-A3B-Instruct-2507",
      "benchmarks": {
        "arena_elo": 1326.0,
        "livebench_global": 42.11,
        "livebench_reasoning": 36.68,
        "livebench_coding": 48.88,
        "livebench_agentic_coding": 1.67,
        "livebench_math": 65.35,
        "livebench_data_analysis": 66.6,
        "livebench_language": 54.47,
        "livebench_ifeval": 21.11
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-VL-32B-Instruct",
      "name": "Qwen/Qwen3-VL-32B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-VL-32B-Instruct"
    },
    {
      "id": "huggingface_inference/ServiceNow-AI/Apriel-1.6-15b-Thinker",
      "name": "ServiceNow-AI/Apriel-1.6-15b-Thinker",
      "provider": "huggingface_inference",
      "model_id": "ServiceNow-AI/Apriel-1.6-15b-Thinker"
    },
    {
      "id": "huggingface_inference/dx8152/Qwen-Edit-2509-Light-Migration",
      "name": "dx8152/Qwen-Edit-2509-Light-Migration",
      "provider": "huggingface_inference",
      "model_id": "dx8152/Qwen-Edit-2509-Light-Migration"
    },
    {
      "id": "huggingface_inference/google-bert/bert-base-uncased",
      "name": "google-bert/bert-base-uncased",
      "provider": "huggingface_inference",
      "model_id": "google-bert/bert-base-uncased"
    },
    {
      "id": "huggingface_inference/sentence-transformers/all-mpnet-base-v2",
      "name": "sentence-transformers/all-mpnet-base-v2",
      "provider": "huggingface_inference",
      "model_id": "sentence-transformers/all-mpnet-base-v2"
    },
    {
      "id": "huggingface_inference/mistralai/Mistral-7B-Instruct-v0.2",
      "name": "mistralai/Mistral-7B-Instruct-v0.2",
      "provider": "huggingface_inference",
      "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
      "benchmarks": {
        "mmlu": 0.625,
        "gsm8k": 0.354,
        "arena_elo": 1151.0
      }
    },
    {
      "id": "huggingface_inference/meta-llama/Llama-3.2-1B-Instruct",
      "name": "meta-llama/Llama-3.2-1B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Llama-3.2-1B-Instruct"
    },
    {
      "id": "huggingface_inference/meta-llama/Llama-3.3-70B-Instruct",
      "name": "meta-llama/Llama-3.3-70B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "benchmarks": {
        "gpqa_diamond": 0.474,
        "mmlu": 0.863,
        "math_level_5": 0.416,
        "otis_mock_aime_2024_2025": 0.051,
        "simplebench": 0.199
      }
    },
    {
      "id": "huggingface_inference/tencent/HunyuanVideo",
      "name": "tencent/HunyuanVideo",
      "provider": "huggingface_inference",
      "model_id": "tencent/HunyuanVideo"
    },
    {
      "id": "huggingface_inference/Wan-AI/Wan2.1-T2V-1.3B",
      "name": "Wan-AI/Wan2.1-T2V-1.3B",
      "provider": "huggingface_inference",
      "model_id": "Wan-AI/Wan2.1-T2V-1.3B"
    },
    {
      "id": "huggingface_inference/mlabonne/gemma-3-27b-it-abliterated",
      "name": "mlabonne/gemma-3-27b-it-abliterated",
      "provider": "huggingface_inference",
      "model_id": "mlabonne/gemma-3-27b-it-abliterated"
    },
    {
      "id": "huggingface_inference/zai-org/GLM-4.7-FP8",
      "name": "zai-org/GLM-4.7-FP8",
      "provider": "huggingface_inference",
      "model_id": "zai-org/GLM-4.7-FP8",
      "benchmarks": {
        "simplebench": 0.477,
        "livebench_global": 60.73,
        "livebench_reasoning": 59.73,
        "livebench_coding": 73.13,
        "livebench_agentic_coding": 41.67,
        "livebench_math": 76.02,
        "livebench_data_analysis": 73.68,
        "livebench_language": 65.23,
        "livebench_ifeval": 35.66
      }
    },
    {
      "id": "huggingface_inference/google-bert/bert-base-chinese",
      "name": "google-bert/bert-base-chinese",
      "provider": "huggingface_inference",
      "model_id": "google-bert/bert-base-chinese"
    },
    {
      "id": "huggingface_inference/FacebookAI/roberta-base",
      "name": "FacebookAI/roberta-base",
      "provider": "huggingface_inference",
      "model_id": "FacebookAI/roberta-base"
    },
    {
      "id": "huggingface_inference/ProsusAI/finbert",
      "name": "ProsusAI/finbert",
      "provider": "huggingface_inference",
      "model_id": "ProsusAI/finbert"
    },
    {
      "id": "huggingface_inference/intfloat/multilingual-e5-large",
      "name": "intfloat/multilingual-e5-large",
      "provider": "huggingface_inference",
      "model_id": "intfloat/multilingual-e5-large"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-Coder-32B-Instruct",
      "name": "Qwen/Qwen2.5-Coder-32B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-Coder-32B-Instruct",
      "benchmarks": {
        "mmlu": 0.791,
        "gsm8k": 0.911,
        "aider_polyglot": 8.0,
        "arena_elo": 1270.0
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-14B",
      "name": "Qwen/Qwen3-14B",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-14B"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-30B-A3B",
      "name": "Qwen/Qwen3-30B-A3B",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-30B-A3B",
      "benchmarks": {
        "arena_elo": 1326.0,
        "livebench_global": 42.11,
        "livebench_reasoning": 36.68,
        "livebench_coding": 48.88,
        "livebench_agentic_coding": 1.67,
        "livebench_math": 65.35,
        "livebench_data_analysis": 66.6,
        "livebench_language": 54.47,
        "livebench_ifeval": 21.11
      }
    },
    {
      "id": "huggingface_inference/Wan-AI/Wan2.2-TI2V-5B",
      "name": "Wan-AI/Wan2.2-TI2V-5B",
      "provider": "huggingface_inference",
      "model_id": "Wan-AI/Wan2.2-TI2V-5B"
    },
    {
      "id": "huggingface_inference/Wan-AI/Wan2.2-T2V-A14B-Diffusers",
      "name": "Wan-AI/Wan2.2-T2V-A14B-Diffusers",
      "provider": "huggingface_inference",
      "model_id": "Wan-AI/Wan2.2-T2V-A14B-Diffusers"
    },
    {
      "id": "huggingface_inference/moonshotai/Kimi-K2-Instruct-0905",
      "name": "moonshotai/Kimi-K2-Instruct-0905",
      "provider": "huggingface_inference",
      "model_id": "moonshotai/Kimi-K2-Instruct-0905",
      "benchmarks": {
        "aider_polyglot": 59.1,
        "simplebench": 0.263,
        "livebench_global": 50.97,
        "livebench_reasoning": 42.23,
        "livebench_coding": 74.28,
        "livebench_agentic_coding": 31.67,
        "livebench_math": 58.15,
        "livebench_data_analysis": 63.41,
        "livebench_language": 66.69,
        "livebench_ifeval": 20.36
      }
    },
    {
      "id": "huggingface_inference/MiniMaxAI/MiniMax-M2",
      "name": "MiniMaxAI/MiniMax-M2",
      "provider": "huggingface_inference",
      "model_id": "MiniMaxAI/MiniMax-M2"
    },
    {
      "id": "huggingface_inference/tlennon-ie/qwen-edit-skin",
      "name": "tlennon-ie/qwen-edit-skin",
      "provider": "huggingface_inference",
      "model_id": "tlennon-ie/qwen-edit-skin"
    },
    {
      "id": "huggingface_inference/zai-org/AutoGLM-Phone-9B-Multilingual",
      "name": "zai-org/AutoGLM-Phone-9B-Multilingual",
      "provider": "huggingface_inference",
      "model_id": "zai-org/AutoGLM-Phone-9B-Multilingual"
    },
    {
      "id": "huggingface_inference/WarmBloodAban/Anything_to_Real_Characters_2511",
      "name": "WarmBloodAban/Anything_to_Real_Characters_2511",
      "provider": "huggingface_inference",
      "model_id": "WarmBloodAban/Anything_to_Real_Characters_2511"
    },
    {
      "id": "huggingface_inference/medicalai/ClinicalBERT",
      "name": "medicalai/ClinicalBERT",
      "provider": "huggingface_inference",
      "model_id": "medicalai/ClinicalBERT"
    },
    {
      "id": "huggingface_inference/intfloat/multilingual-e5-small",
      "name": "intfloat/multilingual-e5-small",
      "provider": "huggingface_inference",
      "model_id": "intfloat/multilingual-e5-small"
    },
    {
      "id": "huggingface_inference/HuggingFaceH4/zephyr-7b-beta",
      "name": "HuggingFaceH4/zephyr-7b-beta",
      "provider": "huggingface_inference",
      "model_id": "HuggingFaceH4/zephyr-7b-beta"
    },
    {
      "id": "huggingface_inference/Sao10K/L3-8B-Stheno-v3.2",
      "name": "Sao10K/L3-8B-Stheno-v3.2",
      "provider": "huggingface_inference",
      "model_id": "Sao10K/L3-8B-Stheno-v3.2"
    },
    {
      "id": "huggingface_inference/aifeifei798/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored",
      "name": "aifeifei798/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored",
      "provider": "huggingface_inference",
      "model_id": "aifeifei798/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored"
    },
    {
      "id": "huggingface_inference/DeepHat/DeepHat-V1-7B",
      "name": "DeepHat/DeepHat-V1-7B",
      "provider": "huggingface_inference",
      "model_id": "DeepHat/DeepHat-V1-7B"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-1.7B",
      "name": "Qwen/Qwen3-1.7B",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-1.7B"
    },
    {
      "id": "huggingface_inference/darkc0de/XortronCriminalComputingConfig",
      "name": "darkc0de/XortronCriminalComputingConfig",
      "provider": "huggingface_inference",
      "model_id": "darkc0de/XortronCriminalComputingConfig"
    },
    {
      "id": "huggingface_inference/google-bert/bert-base-multilingual-cased",
      "name": "google-bert/bert-base-multilingual-cased",
      "provider": "huggingface_inference",
      "model_id": "google-bert/bert-base-multilingual-cased"
    },
    {
      "id": "huggingface_inference/FacebookAI/roberta-large",
      "name": "FacebookAI/roberta-large",
      "provider": "huggingface_inference",
      "model_id": "FacebookAI/roberta-large"
    },
    {
      "id": "huggingface_inference/FacebookAI/xlm-roberta-large",
      "name": "FacebookAI/xlm-roberta-large",
      "provider": "huggingface_inference",
      "model_id": "FacebookAI/xlm-roberta-large"
    },
    {
      "id": "huggingface_inference/MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
      "name": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
      "provider": "huggingface_inference",
      "model_id": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli"
    },
    {
      "id": "huggingface_inference/google/vit-base-patch16-224",
      "name": "google/vit-base-patch16-224",
      "provider": "huggingface_inference",
      "model_id": "google/vit-base-patch16-224"
    },
    {
      "id": "huggingface_inference/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis",
      "name": "mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis",
      "provider": "huggingface_inference",
      "model_id": "mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis"
    },
    {
      "id": "huggingface_inference/Gryphe/MythoMax-L2-13b",
      "name": "Gryphe/MythoMax-L2-13b",
      "provider": "huggingface_inference",
      "model_id": "Gryphe/MythoMax-L2-13b"
    },
    {
      "id": "huggingface_inference/BAAI/bge-large-zh-v1.5",
      "name": "BAAI/bge-large-zh-v1.5",
      "provider": "huggingface_inference",
      "model_id": "BAAI/bge-large-zh-v1.5"
    },
    {
      "id": "huggingface_inference/ByteDance/Hyper-SD",
      "name": "ByteDance/Hyper-SD",
      "provider": "huggingface_inference",
      "model_id": "ByteDance/Hyper-SD"
    },
    {
      "id": "huggingface_inference/meta-llama/Prompt-Guard-86M",
      "name": "meta-llama/Prompt-Guard-86M",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Prompt-Guard-86M"
    },
    {
      "id": "huggingface_inference/NousResearch/Hermes-3-Llama-3.1-8B",
      "name": "NousResearch/Hermes-3-Llama-3.1-8B",
      "provider": "huggingface_inference",
      "model_id": "NousResearch/Hermes-3-Llama-3.1-8B"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-72B-Instruct",
      "name": "Qwen/Qwen2.5-72B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-72B-Instruct",
      "benchmarks": {
        "gpqa_diamond": 0.491,
        "mmlu": 0.834,
        "math_level_5": 0.632,
        "otis_mock_aime_2024_2025": 0.081,
        "trivia_qa": 0.719,
        "arena_elo": 1302.0
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-1.5B-Instruct",
      "name": "Qwen/Qwen2.5-1.5B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-1.5B-Instruct"
    },
    {
      "id": "huggingface_inference/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
      "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
      "provider": "huggingface_inference",
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
      "benchmarks": {
        "gpqa_diamond": 0.447,
        "math_level_5": 0.871
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-VL-72B-Instruct",
      "name": "Qwen/Qwen2.5-VL-72B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-VL-72B-Instruct"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-Next-80B-A3B-Instruct",
      "name": "Qwen/Qwen3-Next-80B-A3B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-Next-80B-A3B-Instruct",
      "benchmarks": {
        "arena_elo": 1401.0,
        "livebench_global": 51.04,
        "livebench_reasoning": 54.75,
        "livebench_coding": 68.2,
        "livebench_agentic_coding": 10.0,
        "livebench_math": 70.18,
        "livebench_data_analysis": 68.63,
        "livebench_language": 66.34,
        "livebench_ifeval": 19.19
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-Next-80B-A3B-Thinking",
      "name": "Qwen/Qwen3-Next-80B-A3B-Thinking",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-Next-80B-A3B-Thinking",
      "benchmarks": {
        "arena_elo": 1368.0,
        "livebench_global": 53.2,
        "livebench_reasoning": 58.16,
        "livebench_coding": 60.66,
        "livebench_agentic_coding": 8.33,
        "livebench_math": 74.26,
        "livebench_data_analysis": 73.16,
        "livebench_language": 56.31,
        "livebench_ifeval": 41.54
      }
    },
    {
      "id": "huggingface_inference/zai-org/GLM-4.6",
      "name": "zai-org/GLM-4.6",
      "provider": "huggingface_inference",
      "model_id": "zai-org/GLM-4.6",
      "benchmarks": {
        "frontiermath": 0.038,
        "frontiermath_tier_4": 0.021,
        "arena_elo": 1424.0,
        "livebench_global": 58.02,
        "livebench_reasoning": 62.06,
        "livebench_coding": 71.02,
        "livebench_agentic_coding": 35.0,
        "livebench_math": 81.13,
        "livebench_data_analysis": 71.74,
        "livebench_language": 58.99,
        "livebench_ifeval": 26.19
      }
    },
    {
      "id": "huggingface_inference/akhaliq/veo3.1-fast-image-to-video",
      "name": "akhaliq/veo3.1-fast-image-to-video",
      "provider": "huggingface_inference",
      "model_id": "akhaliq/veo3.1-fast-image-to-video"
    },
    {
      "id": "huggingface_inference/vafipas663/Qwen-Edit-2509-Upscale-LoRA",
      "name": "vafipas663/Qwen-Edit-2509-Upscale-LoRA",
      "provider": "huggingface_inference",
      "model_id": "vafipas663/Qwen-Edit-2509-Upscale-LoRA"
    },
    {
      "id": "huggingface_inference/tarn59/pixel_art_style_lora_z_image_turbo",
      "name": "tarn59/pixel_art_style_lora_z_image_turbo",
      "provider": "huggingface_inference",
      "model_id": "tarn59/pixel_art_style_lora_z_image_turbo"
    },
    {
      "id": "huggingface_inference/meituan-longcat/LongCat-Image-Edit",
      "name": "meituan-longcat/LongCat-Image-Edit",
      "provider": "huggingface_inference",
      "model_id": "meituan-longcat/LongCat-Image-Edit"
    },
    {
      "id": "huggingface_inference/zai-org/GLM-4.6V",
      "name": "zai-org/GLM-4.6V",
      "provider": "huggingface_inference",
      "model_id": "zai-org/GLM-4.6V",
      "benchmarks": {
        "arena_elo": 1353.0,
        "livebench_global": 42.94,
        "livebench_reasoning": 37.22,
        "livebench_coding": 64.24,
        "livebench_agentic_coding": 3.33,
        "livebench_math": 62.5,
        "livebench_data_analysis": 66.49,
        "livebench_language": 49.74,
        "livebench_ifeval": 17.06
      }
    },
    {
      "id": "huggingface_inference/allenai/Olmo-3.1-32B-Think",
      "name": "allenai/Olmo-3.1-32B-Think",
      "provider": "huggingface_inference",
      "model_id": "allenai/Olmo-3.1-32B-Think"
    },
    {
      "id": "huggingface_inference/starsfriday/Qwen-Image-Edit-2511-Upscale2K",
      "name": "starsfriday/Qwen-Image-Edit-2511-Upscale2K",
      "provider": "huggingface_inference",
      "model_id": "starsfriday/Qwen-Image-Edit-2511-Upscale2K"
    },
    {
      "id": "huggingface_inference/google-bert/bert-base-multilingual-uncased",
      "name": "google-bert/bert-base-multilingual-uncased",
      "provider": "huggingface_inference",
      "model_id": "google-bert/bert-base-multilingual-uncased"
    },
    {
      "id": "huggingface_inference/FacebookAI/xlm-roberta-base",
      "name": "FacebookAI/xlm-roberta-base",
      "provider": "huggingface_inference",
      "model_id": "FacebookAI/xlm-roberta-base"
    },
    {
      "id": "huggingface_inference/dslim/bert-base-NER",
      "name": "dslim/bert-base-NER",
      "provider": "huggingface_inference",
      "model_id": "dslim/bert-base-NER"
    },
    {
      "id": "huggingface_inference/microsoft/deberta-v3-base",
      "name": "microsoft/deberta-v3-base",
      "provider": "huggingface_inference",
      "model_id": "microsoft/deberta-v3-base"
    },
    {
      "id": "huggingface_inference/nateraw/bert-base-uncased-emotion",
      "name": "nateraw/bert-base-uncased-emotion",
      "provider": "huggingface_inference",
      "model_id": "nateraw/bert-base-uncased-emotion"
    },
    {
      "id": "huggingface_inference/nlptown/bert-base-multilingual-uncased-sentiment",
      "name": "nlptown/bert-base-multilingual-uncased-sentiment",
      "provider": "huggingface_inference",
      "model_id": "nlptown/bert-base-multilingual-uncased-sentiment"
    },
    {
      "id": "huggingface_inference/cardiffnlp/twitter-roberta-base-sentiment-latest",
      "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
      "provider": "huggingface_inference",
      "model_id": "cardiffnlp/twitter-roberta-base-sentiment-latest"
    },
    {
      "id": "huggingface_inference/AnnaWegmann/Style-Embedding",
      "name": "AnnaWegmann/Style-Embedding",
      "provider": "huggingface_inference",
      "model_id": "AnnaWegmann/Style-Embedding"
    },
    {
      "id": "huggingface_inference/SamLowe/roberta-base-go_emotions",
      "name": "SamLowe/roberta-base-go_emotions",
      "provider": "huggingface_inference",
      "model_id": "SamLowe/roberta-base-go_emotions"
    },
    {
      "id": "huggingface_inference/intfloat/multilingual-e5-base",
      "name": "intfloat/multilingual-e5-base",
      "provider": "huggingface_inference",
      "model_id": "intfloat/multilingual-e5-base"
    },
    {
      "id": "huggingface_inference/BAAI/bge-base-en-v1.5",
      "name": "BAAI/bge-base-en-v1.5",
      "provider": "huggingface_inference",
      "model_id": "BAAI/bge-base-en-v1.5"
    },
    {
      "id": "huggingface_inference/NeuML/pubmedbert-base-embeddings",
      "name": "NeuML/pubmedbert-base-embeddings",
      "provider": "huggingface_inference",
      "model_id": "NeuML/pubmedbert-base-embeddings"
    },
    {
      "id": "huggingface_inference/mistralai/Mixtral-8x7B-Instruct-v0.1",
      "name": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "provider": "huggingface_inference",
      "model_id": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "benchmarks": {
        "gpqa_diamond": 0.306,
        "mmlu": 0.705,
        "math_level_5": 0.093,
        "gsm8k": 0.744,
        "trivia_qa": 0.822,
        "arena_elo": 1198.0
      }
    },
    {
      "id": "huggingface_inference/intfloat/multilingual-e5-large-instruct",
      "name": "intfloat/multilingual-e5-large-instruct",
      "provider": "huggingface_inference",
      "model_id": "intfloat/multilingual-e5-large-instruct"
    },
    {
      "id": "huggingface_inference/stabilityai/stable-diffusion-3-medium",
      "name": "stabilityai/stable-diffusion-3-medium",
      "provider": "huggingface_inference",
      "model_id": "stabilityai/stable-diffusion-3-medium"
    },
    {
      "id": "huggingface_inference/stabilityai/stable-diffusion-3-medium-diffusers",
      "name": "stabilityai/stable-diffusion-3-medium-diffusers",
      "provider": "huggingface_inference",
      "model_id": "stabilityai/stable-diffusion-3-medium-diffusers"
    },
    {
      "id": "huggingface_inference/google/gemma-2-9b-it",
      "name": "google/gemma-2-9b-it",
      "provider": "huggingface_inference",
      "model_id": "google/gemma-2-9b-it",
      "benchmarks": {
        "gpqa_diamond": 0.275,
        "mmlu": 0.721,
        "math_level_5": 0.21,
        "otis_mock_aime_2024_2025": 0.006,
        "arena_elo": 1265.0
      }
    },
    {
      "id": "huggingface_inference/meta-llama/Llama-Guard-3-8B",
      "name": "meta-llama/Llama-Guard-3-8B",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Llama-Guard-3-8B"
    },
    {
      "id": "huggingface_inference/Shakker-Labs/FLUX.1-dev-LoRA-Logo-Design",
      "name": "Shakker-Labs/FLUX.1-dev-LoRA-Logo-Design",
      "provider": "huggingface_inference",
      "model_id": "Shakker-Labs/FLUX.1-dev-LoRA-Logo-Design"
    },
    {
      "id": "huggingface_inference/inflatebot/MN-12B-Mag-Mell-R1",
      "name": "inflatebot/MN-12B-Mag-Mell-R1",
      "provider": "huggingface_inference",
      "model_id": "inflatebot/MN-12B-Mag-Mell-R1"
    },
    {
      "id": "huggingface_inference/Snowflake/snowflake-arctic-embed-l-v2.0",
      "name": "Snowflake/snowflake-arctic-embed-l-v2.0",
      "provider": "huggingface_inference",
      "model_id": "Snowflake/snowflake-arctic-embed-l-v2.0"
    },
    {
      "id": "huggingface_inference/deepseek-ai/DeepSeek-V3",
      "name": "deepseek-ai/DeepSeek-V3",
      "provider": "huggingface_inference",
      "model_id": "deepseek-ai/DeepSeek-V3",
      "benchmarks": {
        "gpqa_diamond": 0.565,
        "mmlu": 0.871,
        "math_level_5": 0.649,
        "otis_mock_aime_2024_2025": 0.158,
        "frontiermath": 0.017,
        "aider_polyglot": 17.8,
        "simplebench": 0.189,
        "trivia_qa": 0.829,
        "swe_bench_verified": 0.521,
        "arena_elo": 1306.0
      }
    },
    {
      "id": "huggingface_inference/omarelshehy/Arabic-STS-Matryoshka-V2",
      "name": "omarelshehy/Arabic-STS-Matryoshka-V2",
      "provider": "huggingface_inference",
      "model_id": "omarelshehy/Arabic-STS-Matryoshka-V2"
    },
    {
      "id": "huggingface_inference/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
      "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
      "provider": "huggingface_inference",
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-VL-7B-Instruct",
      "name": "Qwen/Qwen2.5-VL-7B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-VL-7B-Instruct"
    },
    {
      "id": "huggingface_inference/fakespot-ai/roberta-base-ai-text-detection-v1",
      "name": "fakespot-ai/roberta-base-ai-text-detection-v1",
      "provider": "huggingface_inference",
      "model_id": "fakespot-ai/roberta-base-ai-text-detection-v1"
    },
    {
      "id": "huggingface_inference/Ateeqq/ai-vs-human-image-detector",
      "name": "Ateeqq/ai-vs-human-image-detector",
      "provider": "huggingface_inference",
      "model_id": "Ateeqq/ai-vs-human-image-detector"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-235B-A22B",
      "name": "Qwen/Qwen3-235B-A22B",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-235B-A22B",
      "benchmarks": {
        "gpqa_diamond": 0.707,
        "math_level_5": 0.689,
        "aider_polyglot": 59.6,
        "simplebench": 0.31,
        "arena_elo": 1374.0
      }
    },
    {
      "id": "huggingface_inference/ibm-granite/granite-embedding-english-r2",
      "name": "ibm-granite/granite-embedding-english-r2",
      "provider": "huggingface_inference",
      "model_id": "ibm-granite/granite-embedding-english-r2"
    },
    {
      "id": "huggingface_inference/peteromallet/Flux-Kontext-InScene",
      "name": "peteromallet/Flux-Kontext-InScene",
      "provider": "huggingface_inference",
      "model_id": "peteromallet/Flux-Kontext-InScene"
    },
    {
      "id": "huggingface_inference/deepseek-ai/DeepSeek-V3.2-Exp",
      "name": "deepseek-ai/DeepSeek-V3.2-Exp",
      "provider": "huggingface_inference",
      "model_id": "deepseek-ai/DeepSeek-V3.2-Exp",
      "benchmarks": {
        "aider_polyglot": 70.2,
        "arena_elo": 1423.0,
        "livebench_global": 52.82,
        "livebench_reasoning": 45.5,
        "livebench_coding": 73.19,
        "livebench_agentic_coding": 36.67,
        "livebench_math": 64.38,
        "livebench_data_analysis": 65.09,
        "livebench_language": 65.6,
        "livebench_ifeval": 19.33
      }
    },
    {
      "id": "huggingface_inference/dx8152/Qwen-Image-Edit-2509-Light_restoration",
      "name": "dx8152/Qwen-Image-Edit-2509-Light_restoration",
      "provider": "huggingface_inference",
      "model_id": "dx8152/Qwen-Image-Edit-2509-Light_restoration"
    },
    {
      "id": "huggingface_inference/dx8152/Qwen-Edit-2509-Multi-Angle-Lighting",
      "name": "dx8152/Qwen-Edit-2509-Multi-Angle-Lighting",
      "provider": "huggingface_inference",
      "model_id": "dx8152/Qwen-Edit-2509-Multi-Angle-Lighting"
    },
    {
      "id": "huggingface_inference/bdsqlsz/qinglong_DetailedEyes_Z-Image",
      "name": "bdsqlsz/qinglong_DetailedEyes_Z-Image",
      "provider": "huggingface_inference",
      "model_id": "bdsqlsz/qinglong_DetailedEyes_Z-Image"
    },
    {
      "id": "huggingface_inference/lovis93/Flux-2-Multi-Angles-LoRA-v2",
      "name": "lovis93/Flux-2-Multi-Angles-LoRA-v2",
      "provider": "huggingface_inference",
      "model_id": "lovis93/Flux-2-Multi-Angles-LoRA-v2"
    },
    {
      "id": "huggingface_inference/Shakker-Labs/AWPortrait-Z",
      "name": "Shakker-Labs/AWPortrait-Z",
      "provider": "huggingface_inference",
      "model_id": "Shakker-Labs/AWPortrait-Z"
    },
    {
      "id": "huggingface_inference/strangerzonehf/Qwen-Image-Edit-LoRA-Collection",
      "name": "strangerzonehf/Qwen-Image-Edit-LoRA-Collection",
      "provider": "huggingface_inference",
      "model_id": "strangerzonehf/Qwen-Image-Edit-LoRA-Collection"
    },
    {
      "id": "huggingface_inference/prithivMLmods/Qwen-Image-Edit-2511-Polaroid-Photo",
      "name": "prithivMLmods/Qwen-Image-Edit-2511-Polaroid-Photo",
      "provider": "huggingface_inference",
      "model_id": "prithivMLmods/Qwen-Image-Edit-2511-Polaroid-Photo"
    },
    {
      "id": "huggingface_inference/google-bert/bert-base-cased",
      "name": "google-bert/bert-base-cased",
      "provider": "huggingface_inference",
      "model_id": "google-bert/bert-base-cased"
    },
    {
      "id": "huggingface_inference/google-bert/bert-large-uncased-whole-word-masking-finetuned-squad",
      "name": "google-bert/bert-large-uncased-whole-word-masking-finetuned-squad",
      "provider": "huggingface_inference",
      "model_id": "google-bert/bert-large-uncased-whole-word-masking-finetuned-squad"
    },
    {
      "id": "huggingface_inference/distilbert/distilroberta-base",
      "name": "distilbert/distilroberta-base",
      "provider": "huggingface_inference",
      "model_id": "distilbert/distilroberta-base"
    },
    {
      "id": "huggingface_inference/openai-community/roberta-base-openai-detector",
      "name": "openai-community/roberta-base-openai-detector",
      "provider": "huggingface_inference",
      "model_id": "openai-community/roberta-base-openai-detector"
    },
    {
      "id": "huggingface_inference/FacebookAI/roberta-large-mnli",
      "name": "FacebookAI/roberta-large-mnli",
      "provider": "huggingface_inference",
      "model_id": "FacebookAI/roberta-large-mnli"
    },
    {
      "id": "huggingface_inference/VoVanPhuc/sup-SimCSE-VietNamese-phobert-base",
      "name": "VoVanPhuc/sup-SimCSE-VietNamese-phobert-base",
      "provider": "huggingface_inference",
      "model_id": "VoVanPhuc/sup-SimCSE-VietNamese-phobert-base"
    },
    {
      "id": "huggingface_inference/facebook/bart-large-mnli",
      "name": "facebook/bart-large-mnli",
      "provider": "huggingface_inference",
      "model_id": "facebook/bart-large-mnli"
    },
    {
      "id": "huggingface_inference/facebook/detr-resnet-50",
      "name": "facebook/detr-resnet-50",
      "provider": "huggingface_inference",
      "model_id": "facebook/detr-resnet-50"
    },
    {
      "id": "huggingface_inference/sagorsarker/bangla-bert-base",
      "name": "sagorsarker/bangla-bert-base",
      "provider": "huggingface_inference",
      "model_id": "sagorsarker/bangla-bert-base"
    },
    {
      "id": "huggingface_inference/sentence-transformers/all-MiniLM-L12-v2",
      "name": "sentence-transformers/all-MiniLM-L12-v2",
      "provider": "huggingface_inference",
      "model_id": "sentence-transformers/all-MiniLM-L12-v2"
    },
    {
      "id": "huggingface_inference/sentence-transformers/multi-qa-mpnet-base-dot-v1",
      "name": "sentence-transformers/multi-qa-mpnet-base-dot-v1",
      "provider": "huggingface_inference",
      "model_id": "sentence-transformers/multi-qa-mpnet-base-dot-v1"
    },
    {
      "id": "huggingface_inference/uer/roberta-base-finetuned-dianping-chinese",
      "name": "uer/roberta-base-finetuned-dianping-chinese",
      "provider": "huggingface_inference",
      "model_id": "uer/roberta-base-finetuned-dianping-chinese"
    },
    {
      "id": "huggingface_inference/jonathandinu/face-parsing",
      "name": "jonathandinu/face-parsing",
      "provider": "huggingface_inference",
      "model_id": "jonathandinu/face-parsing"
    },
    {
      "id": "huggingface_inference/intfloat/e5-base-v2",
      "name": "intfloat/e5-base-v2",
      "provider": "huggingface_inference",
      "model_id": "intfloat/e5-base-v2"
    },
    {
      "id": "huggingface_inference/Minej/bert-base-personality",
      "name": "Minej/bert-base-personality",
      "provider": "huggingface_inference",
      "model_id": "Minej/bert-base-personality"
    },
    {
      "id": "huggingface_inference/BAAI/bge-small-en",
      "name": "BAAI/bge-small-en",
      "provider": "huggingface_inference",
      "model_id": "BAAI/bge-small-en"
    },
    {
      "id": "huggingface_inference/BAAI/bge-large-en-v1.5",
      "name": "BAAI/bge-large-en-v1.5",
      "provider": "huggingface_inference",
      "model_id": "BAAI/bge-large-en-v1.5"
    },
    {
      "id": "huggingface_inference/BAAI/bge-reranker-large",
      "name": "BAAI/bge-reranker-large",
      "provider": "huggingface_inference",
      "model_id": "BAAI/bge-reranker-large"
    },
    {
      "id": "huggingface_inference/protectai/deberta-v3-base-prompt-injection",
      "name": "protectai/deberta-v3-base-prompt-injection",
      "provider": "huggingface_inference",
      "model_id": "protectai/deberta-v3-base-prompt-injection"
    },
    {
      "id": "huggingface_inference/ealvaradob/bert-finetuned-phishing",
      "name": "ealvaradob/bert-finetuned-phishing",
      "provider": "huggingface_inference",
      "model_id": "ealvaradob/bert-finetuned-phishing"
    },
    {
      "id": "huggingface_inference/dima806/skin_types_image_detection",
      "name": "dima806/skin_types_image_detection",
      "provider": "huggingface_inference",
      "model_id": "dima806/skin_types_image_detection"
    },
    {
      "id": "huggingface_inference/mixedbread-ai/mxbai-embed-large-v1",
      "name": "mixedbread-ai/mxbai-embed-large-v1",
      "provider": "huggingface_inference",
      "model_id": "mixedbread-ai/mxbai-embed-large-v1"
    },
    {
      "id": "huggingface_inference/MoritzLaurer/deberta-v3-large-zeroshot-v2.0",
      "name": "MoritzLaurer/deberta-v3-large-zeroshot-v2.0",
      "provider": "huggingface_inference",
      "model_id": "MoritzLaurer/deberta-v3-large-zeroshot-v2.0"
    },
    {
      "id": "huggingface_inference/meta-llama/Meta-Llama-3-70B-Instruct",
      "name": "meta-llama/Meta-Llama-3-70B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Meta-Llama-3-70B-Instruct",
      "benchmarks": {
        "gpqa_diamond": 0.406,
        "mmlu": 0.793,
        "math_level_5": 0.226,
        "otis_mock_aime_2024_2025": 0.043,
        "arena_elo": 1276.0
      }
    },
    {
      "id": "huggingface_inference/aaditya/Llama3-OpenBioLLM-8B",
      "name": "aaditya/Llama3-OpenBioLLM-8B",
      "provider": "huggingface_inference",
      "model_id": "aaditya/Llama3-OpenBioLLM-8B"
    },
    {
      "id": "huggingface_inference/sayeed99/segformer-b3-fashion",
      "name": "sayeed99/segformer-b3-fashion",
      "provider": "huggingface_inference",
      "model_id": "sayeed99/segformer-b3-fashion"
    },
    {
      "id": "huggingface_inference/yentinglin/Llama-3-Taiwan-70B-Instruct",
      "name": "yentinglin/Llama-3-Taiwan-70B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "yentinglin/Llama-3-Taiwan-70B-Instruct"
    },
    {
      "id": "huggingface_inference/fofr/sdxl-emoji",
      "name": "fofr/sdxl-emoji",
      "provider": "huggingface_inference",
      "model_id": "fofr/sdxl-emoji"
    },
    {
      "id": "huggingface_inference/elyza/Llama-3-ELYZA-JP-8B",
      "name": "elyza/Llama-3-ELYZA-JP-8B",
      "provider": "huggingface_inference",
      "model_id": "elyza/Llama-3-ELYZA-JP-8B"
    },
    {
      "id": "huggingface_inference/meta-llama/Llama-3.1-70B-Instruct",
      "name": "meta-llama/Llama-3.1-70B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Llama-3.1-70B-Instruct",
      "benchmarks": {
        "gpqa_diamond": 0.442,
        "mmlu": 0.801,
        "math_level_5": 0.367,
        "otis_mock_aime_2024_2025": 0.036
      }
    },
    {
      "id": "huggingface_inference/mistralai/Mistral-Nemo-Instruct-2407",
      "name": "mistralai/Mistral-Nemo-Instruct-2407",
      "provider": "huggingface_inference",
      "model_id": "mistralai/Mistral-Nemo-Instruct-2407"
    },
    {
      "id": "huggingface_inference/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated",
      "name": "mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated",
      "provider": "huggingface_inference",
      "model_id": "mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated"
    },
    {
      "id": "huggingface_inference/XLabs-AI/flux-RealismLora",
      "name": "XLabs-AI/flux-RealismLora",
      "provider": "huggingface_inference",
      "model_id": "XLabs-AI/flux-RealismLora"
    },
    {
      "id": "huggingface_inference/zai-org/CogVideoX-5b",
      "name": "zai-org/CogVideoX-5b",
      "provider": "huggingface_inference",
      "model_id": "zai-org/CogVideoX-5b"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-1.5B",
      "name": "Qwen/Qwen2.5-1.5B",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-1.5B"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-Coder-3B-Instruct",
      "name": "Qwen/Qwen2.5-Coder-3B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-Coder-3B-Instruct",
      "benchmarks": {
        "gsm8k": 0.757
      }
    },
    {
      "id": "huggingface_inference/tabularisai/multilingual-sentiment-analysis",
      "name": "tabularisai/multilingual-sentiment-analysis",
      "provider": "huggingface_inference",
      "model_id": "tabularisai/multilingual-sentiment-analysis"
    },
    {
      "id": "huggingface_inference/jinaai/ReaderLM-v2",
      "name": "jinaai/ReaderLM-v2",
      "provider": "huggingface_inference",
      "model_id": "jinaai/ReaderLM-v2"
    },
    {
      "id": "huggingface_inference/Steelskull/L3.3-MS-Nevoria-70b",
      "name": "Steelskull/L3.3-MS-Nevoria-70b",
      "provider": "huggingface_inference",
      "model_id": "Steelskull/L3.3-MS-Nevoria-70b"
    },
    {
      "id": "huggingface_inference/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
      "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
      "provider": "huggingface_inference",
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
    },
    {
      "id": "huggingface_inference/PygmalionAI/Eleusis-12B",
      "name": "PygmalionAI/Eleusis-12B",
      "provider": "huggingface_inference",
      "model_id": "PygmalionAI/Eleusis-12B"
    },
    {
      "id": "huggingface_inference/WiroAI/WiroAI-Finance-Qwen-7B",
      "name": "WiroAI/WiroAI-Finance-Qwen-7B",
      "provider": "huggingface_inference",
      "model_id": "WiroAI/WiroAI-Finance-Qwen-7B"
    },
    {
      "id": "huggingface_inference/Wan-AI/Wan2.1-I2V-14B-720P",
      "name": "Wan-AI/Wan2.1-I2V-14B-720P",
      "provider": "huggingface_inference",
      "model_id": "Wan-AI/Wan2.1-I2V-14B-720P"
    },
    {
      "id": "huggingface_inference/Qwen/QwQ-32B",
      "name": "Qwen/QwQ-32B",
      "provider": "huggingface_inference",
      "model_id": "Qwen/QwQ-32B",
      "benchmarks": {
        "aider_polyglot": 20.9,
        "arena_elo": 1159.0
      }
    },
    {
      "id": "huggingface_inference/TheFinAI/StockLLM",
      "name": "TheFinAI/StockLLM",
      "provider": "huggingface_inference",
      "model_id": "TheFinAI/StockLLM"
    },
    {
      "id": "huggingface_inference/redis/langcache-embed-v1",
      "name": "redis/langcache-embed-v1",
      "provider": "huggingface_inference",
      "model_id": "redis/langcache-embed-v1"
    },
    {
      "id": "huggingface_inference/Simonlee711/Clinical_ModernBERT",
      "name": "Simonlee711/Clinical_ModernBERT",
      "provider": "huggingface_inference",
      "model_id": "Simonlee711/Clinical_ModernBERT"
    },
    {
      "id": "huggingface_inference/meta-llama/Llama-4-Maverick-17B-128E-Instruct",
      "name": "meta-llama/Llama-4-Maverick-17B-128E-Instruct",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Llama-4-Maverick-17B-128E-Instruct",
      "benchmarks": {
        "gpqa_diamond": 0.67,
        "math_level_5": 0.73,
        "otis_mock_aime_2024_2025": 0.206,
        "frontiermath": 0.007,
        "aider_polyglot": 15.6,
        "simplebench": 0.277
      }
    },
    {
      "id": "huggingface_inference/zai-org/GLM-4-9B-0414",
      "name": "zai-org/GLM-4-9B-0414",
      "provider": "huggingface_inference",
      "model_id": "zai-org/GLM-4-9B-0414"
    },
    {
      "id": "huggingface_inference/nari-labs/Dia-1.6B",
      "name": "nari-labs/Dia-1.6B",
      "provider": "huggingface_inference",
      "model_id": "nari-labs/Dia-1.6B"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-32B",
      "name": "Qwen/Qwen3-32B",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-32B",
      "benchmarks": {
        "aider_polyglot": 40.0,
        "arena_elo": 1345.0,
        "livebench_global": 46.67,
        "livebench_reasoning": 48.25,
        "livebench_coding": 66.03,
        "livebench_agentic_coding": 3.33,
        "livebench_math": 67.44,
        "livebench_data_analysis": 68.29,
        "livebench_language": 55.54,
        "livebench_ifeval": 17.77
      }
    },
    {
      "id": "huggingface_inference/SentientAGI/Dobby-Mini-Unhinged-Plus-Llama-3.1-8B",
      "name": "SentientAGI/Dobby-Mini-Unhinged-Plus-Llama-3.1-8B",
      "provider": "huggingface_inference",
      "model_id": "SentientAGI/Dobby-Mini-Unhinged-Plus-Llama-3.1-8B"
    },
    {
      "id": "huggingface_inference/thomas-sounack/BioClinical-ModernBERT-base",
      "name": "thomas-sounack/BioClinical-ModernBERT-base",
      "provider": "huggingface_inference",
      "model_id": "thomas-sounack/BioClinical-ModernBERT-base"
    },
    {
      "id": "huggingface_inference/Tongyi-Zhiwen/QwenLong-L1-32B",
      "name": "Tongyi-Zhiwen/QwenLong-L1-32B",
      "provider": "huggingface_inference",
      "model_id": "Tongyi-Zhiwen/QwenLong-L1-32B"
    },
    {
      "id": "huggingface_inference/Unbabel/Tower-Plus-9B",
      "name": "Unbabel/Tower-Plus-9B",
      "provider": "huggingface_inference",
      "model_id": "Unbabel/Tower-Plus-9B"
    },
    {
      "id": "huggingface_inference/zai-org/GLM-4.5-Air",
      "name": "zai-org/GLM-4.5-Air",
      "provider": "huggingface_inference",
      "model_id": "zai-org/GLM-4.5-Air",
      "benchmarks": {
        "arena_elo": 1371.0
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-235B-A22B-Instruct-2507",
      "name": "Qwen/Qwen3-235B-A22B-Instruct-2507",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-235B-A22B-Instruct-2507",
      "benchmarks": {
        "gpqa_diamond": 0.707,
        "math_level_5": 0.689,
        "aider_polyglot": 59.6,
        "simplebench": 0.31,
        "arena_elo": 1374.0
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-Coder-480B-A35B-Instruct",
      "name": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
      "benchmarks": {
        "arena_elo": 1387.0
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8",
      "name": "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8",
      "benchmarks": {
        "arena_elo": 1387.0
      }
    },
    {
      "id": "huggingface_inference/jhu-clsp/mmBERT-base",
      "name": "jhu-clsp/mmBERT-base",
      "provider": "huggingface_inference",
      "model_id": "jhu-clsp/mmBERT-base"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-235B-A22B-Thinking-2507",
      "name": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "benchmarks": {
        "gpqa_diamond": 0.801,
        "otis_mock_aime_2024_2025": 0.867,
        "frontiermath": 0.085,
        "frontiermath_tier_4": 0.0,
        "simpleqa_verified": 0.501,
        "chess_puzzles": 0.122,
        "arena_elo": 1398.0
      }
    },
    {
      "id": "huggingface_inference/Wan-AI/Wan2.2-I2V-A14B-Diffusers",
      "name": "Wan-AI/Wan2.2-I2V-A14B-Diffusers",
      "provider": "huggingface_inference",
      "model_id": "Wan-AI/Wan2.2-I2V-A14B-Diffusers"
    },
    {
      "id": "huggingface_inference/fdtn-ai/Foundation-Sec-8B-Instruct",
      "name": "fdtn-ai/Foundation-Sec-8B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "fdtn-ai/Foundation-Sec-8B-Instruct"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-30B-A3B-Thinking-2507",
      "name": "Qwen/Qwen3-30B-A3B-Thinking-2507",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-30B-A3B-Thinking-2507"
    },
    {
      "id": "huggingface_inference/Lingyuzhou/Qwen_majic_beauty",
      "name": "Lingyuzhou/Qwen_majic_beauty",
      "provider": "huggingface_inference",
      "model_id": "Lingyuzhou/Qwen_majic_beauty"
    },
    {
      "id": "huggingface_inference/jhu-clsp/mmBERT-small",
      "name": "jhu-clsp/mmBERT-small",
      "provider": "huggingface_inference",
      "model_id": "jhu-clsp/mmBERT-small"
    },
    {
      "id": "huggingface_inference/tencent/SRPO",
      "name": "tencent/SRPO",
      "provider": "huggingface_inference",
      "model_id": "tencent/SRPO"
    },
    {
      "id": "huggingface_inference/chetwinlow1/Ovi",
      "name": "chetwinlow1/Ovi",
      "provider": "huggingface_inference",
      "model_id": "chetwinlow1/Ovi"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-VL-30B-A3B-Thinking",
      "name": "Qwen/Qwen3-VL-30B-A3B-Thinking",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-VL-30B-A3B-Thinking"
    },
    {
      "id": "huggingface_inference/dx8152/Qwen-Image-Edit-2509-Relight",
      "name": "dx8152/Qwen-Image-Edit-2509-Relight",
      "provider": "huggingface_inference",
      "model_id": "dx8152/Qwen-Image-Edit-2509-Relight"
    },
    {
      "id": "huggingface_inference/prithivMLmods/Kontext-Watermark-Remover",
      "name": "prithivMLmods/Kontext-Watermark-Remover",
      "provider": "huggingface_inference",
      "model_id": "prithivMLmods/Kontext-Watermark-Remover"
    },
    {
      "id": "huggingface_inference/lrzjason/QwenEdit-Anything2Real_Alpha",
      "name": "lrzjason/QwenEdit-Anything2Real_Alpha",
      "provider": "huggingface_inference",
      "model_id": "lrzjason/QwenEdit-Anything2Real_Alpha"
    },
    {
      "id": "huggingface_inference/allenai/Olmo-3-7B-Think",
      "name": "allenai/Olmo-3-7B-Think",
      "provider": "huggingface_inference",
      "model_id": "allenai/Olmo-3-7B-Think"
    },
    {
      "id": "huggingface_inference/PrimeIntellect/INTELLECT-3-FP8",
      "name": "PrimeIntellect/INTELLECT-3-FP8",
      "provider": "huggingface_inference",
      "model_id": "PrimeIntellect/INTELLECT-3-FP8",
      "benchmarks": {
        "arena_elo": 1354.0
      }
    },
    {
      "id": "huggingface_inference/valiantcat/Qwen-Image-Edit-2509-Upscale2K",
      "name": "valiantcat/Qwen-Image-Edit-2509-Upscale2K",
      "provider": "huggingface_inference",
      "model_id": "valiantcat/Qwen-Image-Edit-2509-Upscale2K"
    },
    {
      "id": "huggingface_inference/ostris/zimage_turbo_training_adapter",
      "name": "ostris/zimage_turbo_training_adapter",
      "provider": "huggingface_inference",
      "model_id": "ostris/zimage_turbo_training_adapter"
    },
    {
      "id": "huggingface_inference/EssentialAI/rnj-1-instruct",
      "name": "EssentialAI/rnj-1-instruct",
      "provider": "huggingface_inference",
      "model_id": "EssentialAI/rnj-1-instruct"
    },
    {
      "id": "huggingface_inference/meituan-longcat/LongCat-Image",
      "name": "meituan-longcat/LongCat-Image",
      "provider": "huggingface_inference",
      "model_id": "meituan-longcat/LongCat-Image"
    },
    {
      "id": "huggingface_inference/utter-project/EuroLLM-22B-Instruct-2512",
      "name": "utter-project/EuroLLM-22B-Instruct-2512",
      "provider": "huggingface_inference",
      "model_id": "utter-project/EuroLLM-22B-Instruct-2512"
    },
    {
      "id": "huggingface_inference/sigstevens33/ASH",
      "name": "sigstevens33/ASH",
      "provider": "huggingface_inference",
      "model_id": "sigstevens33/ASH"
    },
    {
      "id": "huggingface_inference/JunkieMonkey69/StefanieJoosten_ZimageTurbo",
      "name": "JunkieMonkey69/StefanieJoosten_ZimageTurbo",
      "provider": "huggingface_inference",
      "model_id": "JunkieMonkey69/StefanieJoosten_ZimageTurbo"
    },
    {
      "id": "huggingface_inference/jusiflix/UltraRealisticInfluncer",
      "name": "jusiflix/UltraRealisticInfluncer",
      "provider": "huggingface_inference",
      "model_id": "jusiflix/UltraRealisticInfluncer"
    },
    {
      "id": "huggingface_inference/ABDALLALSWAITI/aimaginedworlds",
      "name": "ABDALLALSWAITI/aimaginedworlds",
      "provider": "huggingface_inference",
      "model_id": "ABDALLALSWAITI/aimaginedworlds"
    },
    {
      "id": "huggingface_inference/albert/albert-xxlarge-v2",
      "name": "albert/albert-xxlarge-v2",
      "provider": "huggingface_inference",
      "model_id": "albert/albert-xxlarge-v2"
    },
    {
      "id": "huggingface_inference/google-bert/bert-base-german-dbmdz-uncased",
      "name": "google-bert/bert-base-german-dbmdz-uncased",
      "provider": "huggingface_inference",
      "model_id": "google-bert/bert-base-german-dbmdz-uncased"
    },
    {
      "id": "huggingface_inference/google-bert/bert-large-uncased",
      "name": "google-bert/bert-large-uncased",
      "provider": "huggingface_inference",
      "model_id": "google-bert/bert-large-uncased"
    },
    {
      "id": "huggingface_inference/distilbert/distilbert-base-cased",
      "name": "distilbert/distilbert-base-cased",
      "provider": "huggingface_inference",
      "model_id": "distilbert/distilbert-base-cased"
    },
    {
      "id": "huggingface_inference/distilbert/distilbert-base-uncased-distilled-squad",
      "name": "distilbert/distilbert-base-uncased-distilled-squad",
      "provider": "huggingface_inference",
      "model_id": "distilbert/distilbert-base-uncased-distilled-squad"
    },
    {
      "id": "huggingface_inference/FacebookAI/xlm-roberta-large-finetuned-conll03-english",
      "name": "FacebookAI/xlm-roberta-large-finetuned-conll03-english",
      "provider": "huggingface_inference",
      "model_id": "FacebookAI/xlm-roberta-large-finetuned-conll03-english"
    },
    {
      "id": "huggingface_inference/FacebookAI/xlm-roberta-large-finetuned-conll03-german",
      "name": "FacebookAI/xlm-roberta-large-finetuned-conll03-german",
      "provider": "huggingface_inference",
      "model_id": "FacebookAI/xlm-roberta-large-finetuned-conll03-german"
    },
    {
      "id": "huggingface_inference/GroNLP/hateBERT",
      "name": "GroNLP/hateBERT",
      "provider": "huggingface_inference",
      "model_id": "GroNLP/hateBERT"
    },
    {
      "id": "huggingface_inference/Helsinki-NLP/opus-mt-en-zh",
      "name": "Helsinki-NLP/opus-mt-en-zh",
      "provider": "huggingface_inference",
      "model_id": "Helsinki-NLP/opus-mt-en-zh"
    },
    {
      "id": "huggingface_inference/Helsinki-NLP/opus-mt-ja-en",
      "name": "Helsinki-NLP/opus-mt-ja-en",
      "provider": "huggingface_inference",
      "model_id": "Helsinki-NLP/opus-mt-ja-en"
    },
    {
      "id": "huggingface_inference/Helsinki-NLP/opus-mt-ru-en",
      "name": "Helsinki-NLP/opus-mt-ru-en",
      "provider": "huggingface_inference",
      "model_id": "Helsinki-NLP/opus-mt-ru-en"
    },
    {
      "id": "huggingface_inference/HooshvareLab/bert-base-parsbert-uncased",
      "name": "HooshvareLab/bert-base-parsbert-uncased",
      "provider": "huggingface_inference",
      "model_id": "HooshvareLab/bert-base-parsbert-uncased"
    },
    {
      "id": "huggingface_inference/Jean-Baptiste/camembert-ner-with-dates",
      "name": "Jean-Baptiste/camembert-ner-with-dates",
      "provider": "huggingface_inference",
      "model_id": "Jean-Baptiste/camembert-ner-with-dates"
    },
    {
      "id": "huggingface_inference/airesearch/wangchanberta-base-att-spm-uncased",
      "name": "airesearch/wangchanberta-base-att-spm-uncased",
      "provider": "huggingface_inference",
      "model_id": "airesearch/wangchanberta-base-att-spm-uncased"
    },
    {
      "id": "huggingface_inference/aubmindlab/bert-base-arabertv02",
      "name": "aubmindlab/bert-base-arabertv02",
      "provider": "huggingface_inference",
      "model_id": "aubmindlab/bert-base-arabertv02"
    },
    {
      "id": "huggingface_inference/cahya/bert-base-indonesian-NER",
      "name": "cahya/bert-base-indonesian-NER",
      "provider": "huggingface_inference",
      "model_id": "cahya/bert-base-indonesian-NER"
    },
    {
      "id": "huggingface_inference/cointegrated/rubert-tiny2",
      "name": "cointegrated/rubert-tiny2",
      "provider": "huggingface_inference",
      "model_id": "cointegrated/rubert-tiny2"
    },
    {
      "id": "huggingface_inference/csebuetnlp/mT5_multilingual_XLSum",
      "name": "csebuetnlp/mT5_multilingual_XLSum",
      "provider": "huggingface_inference",
      "model_id": "csebuetnlp/mT5_multilingual_XLSum"
    },
    {
      "id": "huggingface_inference/dccuchile/bert-base-spanish-wwm-uncased",
      "name": "dccuchile/bert-base-spanish-wwm-uncased",
      "provider": "huggingface_inference",
      "model_id": "dccuchile/bert-base-spanish-wwm-uncased"
    },
    {
      "id": "huggingface_inference/deepset/minilm-uncased-squad2",
      "name": "deepset/minilm-uncased-squad2",
      "provider": "huggingface_inference",
      "model_id": "deepset/minilm-uncased-squad2"
    },
    {
      "id": "huggingface_inference/emilyalsentzer/Bio_ClinicalBERT",
      "name": "emilyalsentzer/Bio_ClinicalBERT",
      "provider": "huggingface_inference",
      "model_id": "emilyalsentzer/Bio_ClinicalBERT"
    },
    {
      "id": "huggingface_inference/google/pegasus-cnn_dailymail",
      "name": "google/pegasus-cnn_dailymail",
      "provider": "huggingface_inference",
      "model_id": "google/pegasus-cnn_dailymail"
    },
    {
      "id": "huggingface_inference/google/vit-large-patch32-384",
      "name": "google/vit-large-patch32-384",
      "provider": "huggingface_inference",
      "model_id": "google/vit-large-patch32-384"
    },
    {
      "id": "huggingface_inference/hfl/chinese-bert-wwm-ext",
      "name": "hfl/chinese-bert-wwm-ext",
      "provider": "huggingface_inference",
      "model_id": "hfl/chinese-bert-wwm-ext"
    },
    {
      "id": "huggingface_inference/hfl/chinese-roberta-wwm-ext",
      "name": "hfl/chinese-roberta-wwm-ext",
      "provider": "huggingface_inference",
      "model_id": "hfl/chinese-roberta-wwm-ext"
    },
    {
      "id": "huggingface_inference/hiiamsid/sentence_similarity_spanish_es",
      "name": "hiiamsid/sentence_similarity_spanish_es",
      "provider": "huggingface_inference",
      "model_id": "hiiamsid/sentence_similarity_spanish_es"
    },
    {
      "id": "huggingface_inference/huggingface/CodeBERTa-language-id",
      "name": "huggingface/CodeBERTa-language-id",
      "provider": "huggingface_inference",
      "model_id": "huggingface/CodeBERTa-language-id"
    },
    {
      "id": "huggingface_inference/iarfmoose/bert-base-cased-qa-evaluator",
      "name": "iarfmoose/bert-base-cased-qa-evaluator",
      "provider": "huggingface_inference",
      "model_id": "iarfmoose/bert-base-cased-qa-evaluator"
    },
    {
      "id": "huggingface_inference/j-hartmann/emotion-english-distilroberta-base",
      "name": "j-hartmann/emotion-english-distilroberta-base",
      "provider": "huggingface_inference",
      "model_id": "j-hartmann/emotion-english-distilroberta-base"
    },
    {
      "id": "huggingface_inference/jackaduma/SecBERT",
      "name": "jackaduma/SecBERT",
      "provider": "huggingface_inference",
      "model_id": "jackaduma/SecBERT"
    },
    {
      "id": "huggingface_inference/jpwahle/longformer-base-plagiarism-detection",
      "name": "jpwahle/longformer-base-plagiarism-detection",
      "provider": "huggingface_inference",
      "model_id": "jpwahle/longformer-base-plagiarism-detection"
    },
    {
      "id": "huggingface_inference/klue/roberta-large",
      "name": "klue/roberta-large",
      "provider": "huggingface_inference",
      "model_id": "klue/roberta-large"
    },
    {
      "id": "huggingface_inference/knkarthick/MEETING_SUMMARY",
      "name": "knkarthick/MEETING_SUMMARY",
      "provider": "huggingface_inference",
      "model_id": "knkarthick/MEETING_SUMMARY"
    },
    {
      "id": "huggingface_inference/microsoft/deberta-large-mnli",
      "name": "microsoft/deberta-large-mnli",
      "provider": "huggingface_inference",
      "model_id": "microsoft/deberta-large-mnli"
    },
    {
      "id": "huggingface_inference/microsoft/graphcodebert-base",
      "name": "microsoft/graphcodebert-base",
      "provider": "huggingface_inference",
      "model_id": "microsoft/graphcodebert-base"
    },
    {
      "id": "huggingface_inference/microsoft/mdeberta-v3-base",
      "name": "microsoft/mdeberta-v3-base",
      "provider": "huggingface_inference",
      "model_id": "microsoft/mdeberta-v3-base"
    },
    {
      "id": "huggingface_inference/microsoft/swin-large-patch4-window12-384",
      "name": "microsoft/swin-large-patch4-window12-384",
      "provider": "huggingface_inference",
      "model_id": "microsoft/swin-large-patch4-window12-384"
    },
    {
      "id": "huggingface_inference/mrm8488/bert-tiny-finetuned-fake-news-detection",
      "name": "mrm8488/bert-tiny-finetuned-fake-news-detection",
      "provider": "huggingface_inference",
      "model_id": "mrm8488/bert-tiny-finetuned-fake-news-detection"
    },
    {
      "id": "huggingface_inference/nateraw/vit-age-classifier",
      "name": "nateraw/vit-age-classifier",
      "provider": "huggingface_inference",
      "model_id": "nateraw/vit-age-classifier"
    },
    {
      "id": "huggingface_inference/neuralmind/bert-base-portuguese-cased",
      "name": "neuralmind/bert-base-portuguese-cased",
      "provider": "huggingface_inference",
      "model_id": "neuralmind/bert-base-portuguese-cased"
    },
    {
      "id": "huggingface_inference/nlpaueb/legal-bert-base-uncased",
      "name": "nlpaueb/legal-bert-base-uncased",
      "provider": "huggingface_inference",
      "model_id": "nlpaueb/legal-bert-base-uncased"
    },
    {
      "id": "huggingface_inference/nvidia/segformer-b5-finetuned-cityscapes-1024-1024",
      "name": "nvidia/segformer-b5-finetuned-cityscapes-1024-1024",
      "provider": "huggingface_inference",
      "model_id": "nvidia/segformer-b5-finetuned-cityscapes-1024-1024"
    },
    {
      "id": "huggingface_inference/saattrupdan/nbailab-base-ner-scandi",
      "name": "saattrupdan/nbailab-base-ner-scandi",
      "provider": "huggingface_inference",
      "model_id": "saattrupdan/nbailab-base-ner-scandi"
    },
    {
      "id": "huggingface_inference/sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
      "name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
      "provider": "huggingface_inference",
      "model_id": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    },
    {
      "id": "huggingface_inference/seyonec/ChemBERTa_zinc250k_v2_40k",
      "name": "seyonec/ChemBERTa_zinc250k_v2_40k",
      "provider": "huggingface_inference",
      "model_id": "seyonec/ChemBERTa_zinc250k_v2_40k"
    },
    {
      "id": "huggingface_inference/shibing624/text2vec-base-chinese",
      "name": "shibing624/text2vec-base-chinese",
      "provider": "huggingface_inference",
      "model_id": "shibing624/text2vec-base-chinese"
    },
    {
      "id": "huggingface_inference/snunlp/KR-FinBert-SC",
      "name": "snunlp/KR-FinBert-SC",
      "provider": "huggingface_inference",
      "model_id": "snunlp/KR-FinBert-SC"
    },
    {
      "id": "huggingface_inference/uer/sbert-base-chinese-nli",
      "name": "uer/sbert-base-chinese-nli",
      "provider": "huggingface_inference",
      "model_id": "uer/sbert-base-chinese-nli"
    },
    {
      "id": "huggingface_inference/valhalla/distilbart-mnli-12-1",
      "name": "valhalla/distilbart-mnli-12-1",
      "provider": "huggingface_inference",
      "model_id": "valhalla/distilbart-mnli-12-1"
    },
    {
      "id": "huggingface_inference/w11wo/indonesian-roberta-base-sentiment-classifier",
      "name": "w11wo/indonesian-roberta-base-sentiment-classifier",
      "provider": "huggingface_inference",
      "model_id": "w11wo/indonesian-roberta-base-sentiment-classifier"
    },
    {
      "id": "huggingface_inference/microsoft/unixcoder-base",
      "name": "microsoft/unixcoder-base",
      "provider": "huggingface_inference",
      "model_id": "microsoft/unixcoder-base"
    },
    {
      "id": "huggingface_inference/Helsinki-NLP/opus-mt-tc-big-en-fr",
      "name": "Helsinki-NLP/opus-mt-tc-big-en-fr",
      "provider": "huggingface_inference",
      "model_id": "Helsinki-NLP/opus-mt-tc-big-en-fr"
    },
    {
      "id": "huggingface_inference/Helsinki-NLP/opus-mt-tc-big-en-pt",
      "name": "Helsinki-NLP/opus-mt-tc-big-en-pt",
      "provider": "huggingface_inference",
      "model_id": "Helsinki-NLP/opus-mt-tc-big-en-pt"
    },
    {
      "id": "huggingface_inference/microsoft/BiomedVLP-CXR-BERT-general",
      "name": "microsoft/BiomedVLP-CXR-BERT-general",
      "provider": "huggingface_inference",
      "model_id": "microsoft/BiomedVLP-CXR-BERT-general"
    },
    {
      "id": "huggingface_inference/apple/mobilevit-small",
      "name": "apple/mobilevit-small",
      "provider": "huggingface_inference",
      "model_id": "apple/mobilevit-small"
    },
    {
      "id": "huggingface_inference/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli",
      "name": "MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli",
      "provider": "huggingface_inference",
      "model_id": "MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli"
    },
    {
      "id": "huggingface_inference/ElKulako/cryptobert",
      "name": "ElKulako/cryptobert",
      "provider": "huggingface_inference",
      "model_id": "ElKulako/cryptobert"
    },
    {
      "id": "huggingface_inference/IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese",
      "name": "IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese",
      "provider": "huggingface_inference",
      "model_id": "IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese"
    },
    {
      "id": "huggingface_inference/MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7",
      "name": "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7",
      "provider": "huggingface_inference",
      "model_id": "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7"
    },
    {
      "id": "huggingface_inference/kuelumbus/polyBERT",
      "name": "kuelumbus/polyBERT",
      "provider": "huggingface_inference",
      "model_id": "kuelumbus/polyBERT"
    },
    {
      "id": "huggingface_inference/facebook/esm2_t6_8M_UR50D",
      "name": "facebook/esm2_t6_8M_UR50D",
      "provider": "huggingface_inference",
      "model_id": "facebook/esm2_t6_8M_UR50D"
    },
    {
      "id": "huggingface_inference/umm-maybe/AI-image-detector",
      "name": "umm-maybe/AI-image-detector",
      "provider": "huggingface_inference",
      "model_id": "umm-maybe/AI-image-detector"
    },
    {
      "id": "huggingface_inference/ehsanaghaei/SecureBERT",
      "name": "ehsanaghaei/SecureBERT",
      "provider": "huggingface_inference",
      "model_id": "ehsanaghaei/SecureBERT"
    },
    {
      "id": "huggingface_inference/facebook/esm2_t36_3B_UR50D",
      "name": "facebook/esm2_t36_3B_UR50D",
      "provider": "huggingface_inference",
      "model_id": "facebook/esm2_t36_3B_UR50D"
    },
    {
      "id": "huggingface_inference/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb",
      "name": "pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb",
      "provider": "huggingface_inference",
      "model_id": "pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb"
    },
    {
      "id": "huggingface_inference/mattmdjaga/segformer_b2_clothes",
      "name": "mattmdjaga/segformer_b2_clothes",
      "provider": "huggingface_inference",
      "model_id": "mattmdjaga/segformer_b2_clothes"
    },
    {
      "id": "huggingface_inference/facebook/mask2former-swin-base-coco-instance",
      "name": "facebook/mask2former-swin-base-coco-instance",
      "provider": "huggingface_inference",
      "model_id": "facebook/mask2former-swin-base-coco-instance"
    },
    {
      "id": "huggingface_inference/intfloat/e5-small",
      "name": "intfloat/e5-small",
      "provider": "huggingface_inference",
      "model_id": "intfloat/e5-small"
    },
    {
      "id": "huggingface_inference/PirateXX/AI-Content-Detector",
      "name": "PirateXX/AI-Content-Detector",
      "provider": "huggingface_inference",
      "model_id": "PirateXX/AI-Content-Detector"
    },
    {
      "id": "huggingface_inference/whispAI/ClaimBuster-DeBERTaV2",
      "name": "whispAI/ClaimBuster-DeBERTaV2",
      "provider": "huggingface_inference",
      "model_id": "whispAI/ClaimBuster-DeBERTaV2"
    },
    {
      "id": "huggingface_inference/google/efficientnet-b0",
      "name": "google/efficientnet-b0",
      "provider": "huggingface_inference",
      "model_id": "google/efficientnet-b0"
    },
    {
      "id": "huggingface_inference/google/efficientnet-b3",
      "name": "google/efficientnet-b3",
      "provider": "huggingface_inference",
      "model_id": "google/efficientnet-b3"
    },
    {
      "id": "huggingface_inference/facebook/convnextv2-huge-22k-512",
      "name": "facebook/convnextv2-huge-22k-512",
      "provider": "huggingface_inference",
      "model_id": "facebook/convnextv2-huge-22k-512"
    },
    {
      "id": "huggingface_inference/facebook/convnextv2-base-22k-224",
      "name": "facebook/convnextv2-base-22k-224",
      "provider": "huggingface_inference",
      "model_id": "facebook/convnextv2-base-22k-224"
    },
    {
      "id": "huggingface_inference/linkanjarad/mobilenet_v2_1.0_224-plant-disease-identification",
      "name": "linkanjarad/mobilenet_v2_1.0_224-plant-disease-identification",
      "provider": "huggingface_inference",
      "model_id": "linkanjarad/mobilenet_v2_1.0_224-plant-disease-identification"
    },
    {
      "id": "huggingface_inference/lxyuan/distilbert-base-multilingual-cased-sentiments-student",
      "name": "lxyuan/distilbert-base-multilingual-cased-sentiments-student",
      "provider": "huggingface_inference",
      "model_id": "lxyuan/distilbert-base-multilingual-cased-sentiments-student"
    },
    {
      "id": "huggingface_inference/intfloat/e5-small-v2",
      "name": "intfloat/e5-small-v2",
      "provider": "huggingface_inference",
      "model_id": "intfloat/e5-small-v2"
    },
    {
      "id": "huggingface_inference/BAAI/bge-large-zh",
      "name": "BAAI/bge-large-zh",
      "provider": "huggingface_inference",
      "model_id": "BAAI/bge-large-zh"
    },
    {
      "id": "huggingface_inference/nerijs/pixel-art-xl",
      "name": "nerijs/pixel-art-xl",
      "provider": "huggingface_inference",
      "model_id": "nerijs/pixel-art-xl"
    },
    {
      "id": "huggingface_inference/bowphs/SPhilBerta",
      "name": "bowphs/SPhilBerta",
      "provider": "huggingface_inference",
      "model_id": "bowphs/SPhilBerta"
    },
    {
      "id": "huggingface_inference/BAAI/bge-small-en-v1.5",
      "name": "BAAI/bge-small-en-v1.5",
      "provider": "huggingface_inference",
      "model_id": "BAAI/bge-small-en-v1.5"
    },
    {
      "id": "huggingface_inference/BAAI/bge-small-zh-v1.5",
      "name": "BAAI/bge-small-zh-v1.5",
      "provider": "huggingface_inference",
      "model_id": "BAAI/bge-small-zh-v1.5"
    },
    {
      "id": "huggingface_inference/jbochi/madlad400-3b-mt",
      "name": "jbochi/madlad400-3b-mt",
      "provider": "huggingface_inference",
      "model_id": "jbochi/madlad400-3b-mt"
    },
    {
      "id": "huggingface_inference/KoalaAI/Text-Moderation",
      "name": "KoalaAI/Text-Moderation",
      "provider": "huggingface_inference",
      "model_id": "KoalaAI/Text-Moderation"
    },
    {
      "id": "huggingface_inference/TaylorAI/bge-micro-v2",
      "name": "TaylorAI/bge-micro-v2",
      "provider": "huggingface_inference",
      "model_id": "TaylorAI/bge-micro-v2"
    },
    {
      "id": "huggingface_inference/dima806/deepfake_vs_real_image_detection",
      "name": "dima806/deepfake_vs_real_image_detection",
      "provider": "huggingface_inference",
      "model_id": "dima806/deepfake_vs_real_image_detection"
    },
    {
      "id": "huggingface_inference/blink7630/storyboard-sketch",
      "name": "blink7630/storyboard-sketch",
      "provider": "huggingface_inference",
      "model_id": "blink7630/storyboard-sketch"
    },
    {
      "id": "huggingface_inference/FremyCompany/BioLORD-2023-M",
      "name": "FremyCompany/BioLORD-2023-M",
      "provider": "huggingface_inference",
      "model_id": "FremyCompany/BioLORD-2023-M"
    },
    {
      "id": "huggingface_inference/kakaobank/kf-deberta-base",
      "name": "kakaobank/kf-deberta-base",
      "provider": "huggingface_inference",
      "model_id": "kakaobank/kf-deberta-base"
    },
    {
      "id": "huggingface_inference/ntc-ai/SDXL-LoRA-slider.pixel-art",
      "name": "ntc-ai/SDXL-LoRA-slider.pixel-art",
      "provider": "huggingface_inference",
      "model_id": "ntc-ai/SDXL-LoRA-slider.pixel-art"
    },
    {
      "id": "huggingface_inference/ntc-ai/SDXL-LoRA-slider.asleep",
      "name": "ntc-ai/SDXL-LoRA-slider.asleep",
      "provider": "huggingface_inference",
      "model_id": "ntc-ai/SDXL-LoRA-slider.asleep"
    },
    {
      "id": "huggingface_inference/ntc-ai/SDXL-LoRA-slider.sexy",
      "name": "ntc-ai/SDXL-LoRA-slider.sexy",
      "provider": "huggingface_inference",
      "model_id": "ntc-ai/SDXL-LoRA-slider.sexy"
    },
    {
      "id": "huggingface_inference/intfloat/e5-mistral-7b-instruct",
      "name": "intfloat/e5-mistral-7b-instruct",
      "provider": "huggingface_inference",
      "model_id": "intfloat/e5-mistral-7b-instruct"
    },
    {
      "id": "huggingface_inference/imfarzanansari/skintelligent-acne",
      "name": "imfarzanansari/skintelligent-acne",
      "provider": "huggingface_inference",
      "model_id": "imfarzanansari/skintelligent-acne"
    },
    {
      "id": "huggingface_inference/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T",
      "name": "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T",
      "provider": "huggingface_inference",
      "model_id": "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"
    },
    {
      "id": "huggingface_inference/Organika/sdxl-detector",
      "name": "Organika/sdxl-detector",
      "provider": "huggingface_inference",
      "model_id": "Organika/sdxl-detector"
    },
    {
      "id": "huggingface_inference/ntc-ai/SDXL-LoRA-slider.unreal-engine",
      "name": "ntc-ai/SDXL-LoRA-slider.unreal-engine",
      "provider": "huggingface_inference",
      "model_id": "ntc-ai/SDXL-LoRA-slider.unreal-engine"
    },
    {
      "id": "huggingface_inference/ntc-ai/SDXL-LoRA-slider.spritesheet",
      "name": "ntc-ai/SDXL-LoRA-slider.spritesheet",
      "provider": "huggingface_inference",
      "model_id": "ntc-ai/SDXL-LoRA-slider.spritesheet"
    },
    {
      "id": "huggingface_inference/philomath-1209/programming-language-identification",
      "name": "philomath-1209/programming-language-identification",
      "provider": "huggingface_inference",
      "model_id": "philomath-1209/programming-language-identification"
    },
    {
      "id": "huggingface_inference/OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract",
      "name": "OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract",
      "provider": "huggingface_inference",
      "model_id": "OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract"
    },
    {
      "id": "huggingface_inference/MoritzLaurer/xtremedistil-l6-h256-zeroshot-v1.1-all-33",
      "name": "MoritzLaurer/xtremedistil-l6-h256-zeroshot-v1.1-all-33",
      "provider": "huggingface_inference",
      "model_id": "MoritzLaurer/xtremedistil-l6-h256-zeroshot-v1.1-all-33"
    },
    {
      "id": "huggingface_inference/MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33",
      "name": "MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33",
      "provider": "huggingface_inference",
      "model_id": "MoritzLaurer/deberta-v3-xsmall-zeroshot-v1.1-all-33"
    },
    {
      "id": "huggingface_inference/pborchert/BusinessBERT",
      "name": "pborchert/BusinessBERT",
      "provider": "huggingface_inference",
      "model_id": "pborchert/BusinessBERT"
    },
    {
      "id": "huggingface_inference/SanjiWatsuki/Kunoichi-DPO-v2-7B",
      "name": "SanjiWatsuki/Kunoichi-DPO-v2-7B",
      "provider": "huggingface_inference",
      "model_id": "SanjiWatsuki/Kunoichi-DPO-v2-7B"
    },
    {
      "id": "huggingface_inference/Norod78/SDXL-LaundryArt-LoRA-r32",
      "name": "Norod78/SDXL-LaundryArt-LoRA-r32",
      "provider": "huggingface_inference",
      "model_id": "Norod78/SDXL-LaundryArt-LoRA-r32"
    },
    {
      "id": "huggingface_inference/DMetaSoul/Dmeta-embedding-zh",
      "name": "DMetaSoul/Dmeta-embedding-zh",
      "provider": "huggingface_inference",
      "model_id": "DMetaSoul/Dmeta-embedding-zh"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen1.5-1.8B-Chat",
      "name": "Qwen/Qwen1.5-1.8B-Chat",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen1.5-1.8B-Chat"
    },
    {
      "id": "huggingface_inference/blaze999/Medical-NER",
      "name": "blaze999/Medical-NER",
      "provider": "huggingface_inference",
      "model_id": "blaze999/Medical-NER"
    },
    {
      "id": "huggingface_inference/ByteDance/SDXL-Lightning",
      "name": "ByteDance/SDXL-Lightning",
      "provider": "huggingface_inference",
      "model_id": "ByteDance/SDXL-Lightning"
    },
    {
      "id": "huggingface_inference/mpi-inno-comp/paecter",
      "name": "mpi-inno-comp/paecter",
      "provider": "huggingface_inference",
      "model_id": "mpi-inno-comp/paecter"
    },
    {
      "id": "huggingface_inference/h1t/TCD-SDXL-LoRA",
      "name": "h1t/TCD-SDXL-LoRA",
      "provider": "huggingface_inference",
      "model_id": "h1t/TCD-SDXL-LoRA"
    },
    {
      "id": "huggingface_inference/microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
      "name": "microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
      "provider": "huggingface_inference",
      "model_id": "microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank"
    },
    {
      "id": "huggingface_inference/speakleash/Bielik-7B-Instruct-v0.1",
      "name": "speakleash/Bielik-7B-Instruct-v0.1",
      "provider": "huggingface_inference",
      "model_id": "speakleash/Bielik-7B-Instruct-v0.1"
    },
    {
      "id": "huggingface_inference/sWizad/pokemon-trainer-sprite-pixelart",
      "name": "sWizad/pokemon-trainer-sprite-pixelart",
      "provider": "huggingface_inference",
      "model_id": "sWizad/pokemon-trainer-sprite-pixelart"
    },
    {
      "id": "huggingface_inference/alpindale/WizardLM-2-8x22B",
      "name": "alpindale/WizardLM-2-8x22B",
      "provider": "huggingface_inference",
      "model_id": "alpindale/WizardLM-2-8x22B",
      "benchmarks": {
        "gpqa_diamond": 0.434,
        "math_level_5": 0.257
      }
    },
    {
      "id": "huggingface_inference/mistralai/Mixtral-8x22B-Instruct-v0.1",
      "name": "mistralai/Mixtral-8x22B-Instruct-v0.1",
      "provider": "huggingface_inference",
      "model_id": "mistralai/Mixtral-8x22B-Instruct-v0.1",
      "benchmarks": {
        "gpqa_diamond": 0.306,
        "mmlu": 0.705,
        "math_level_5": 0.093,
        "gsm8k": 0.744,
        "trivia_qa": 0.822,
        "arena_elo": 1198.0
      }
    },
    {
      "id": "huggingface_inference/VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct",
      "name": "VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct",
      "provider": "huggingface_inference",
      "model_id": "VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct"
    },
    {
      "id": "huggingface_inference/jeiku/Chaos_RP_l3_8B",
      "name": "jeiku/Chaos_RP_l3_8B",
      "provider": "huggingface_inference",
      "model_id": "jeiku/Chaos_RP_l3_8B"
    },
    {
      "id": "huggingface_inference/FinLang/finance-embeddings-investopedia",
      "name": "FinLang/finance-embeddings-investopedia",
      "provider": "huggingface_inference",
      "model_id": "FinLang/finance-embeddings-investopedia"
    },
    {
      "id": "huggingface_inference/NousResearch/Hermes-2-Pro-Llama-3-8B",
      "name": "NousResearch/Hermes-2-Pro-Llama-3-8B",
      "provider": "huggingface_inference",
      "model_id": "NousResearch/Hermes-2-Pro-Llama-3-8B"
    },
    {
      "id": "huggingface_inference/KevSun/Personality_LM",
      "name": "KevSun/Personality_LM",
      "provider": "huggingface_inference",
      "model_id": "KevSun/Personality_LM"
    },
    {
      "id": "huggingface_inference/failspy/llama-3-70B-Instruct-abliterated",
      "name": "failspy/llama-3-70B-Instruct-abliterated",
      "provider": "huggingface_inference",
      "model_id": "failspy/llama-3-70B-Instruct-abliterated"
    },
    {
      "id": "huggingface_inference/defog/llama-3-sqlcoder-8b",
      "name": "defog/llama-3-sqlcoder-8b",
      "provider": "huggingface_inference",
      "model_id": "defog/llama-3-sqlcoder-8b"
    },
    {
      "id": "huggingface_inference/failspy/Meta-Llama-3-8B-Instruct-abliterated-v3",
      "name": "failspy/Meta-Llama-3-8B-Instruct-abliterated-v3",
      "provider": "huggingface_inference",
      "model_id": "failspy/Meta-Llama-3-8B-Instruct-abliterated-v3"
    },
    {
      "id": "huggingface_inference/dphn/dolphin-2.9.2-qwen2-7b",
      "name": "dphn/dolphin-2.9.2-qwen2-7b",
      "provider": "huggingface_inference",
      "model_id": "dphn/dolphin-2.9.2-qwen2-7b"
    },
    {
      "id": "huggingface_inference/mlabonne/NeuralDaredevil-8B-abliterated",
      "name": "mlabonne/NeuralDaredevil-8B-abliterated",
      "provider": "huggingface_inference",
      "model_id": "mlabonne/NeuralDaredevil-8B-abliterated"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2-7B-Instruct",
      "name": "Qwen/Qwen2-7B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2-7B-Instruct"
    },
    {
      "id": "huggingface_inference/Kwai-Kolors/Kolors",
      "name": "Kwai-Kolors/Kolors",
      "provider": "huggingface_inference",
      "model_id": "Kwai-Kolors/Kolors"
    },
    {
      "id": "huggingface_inference/Sao10K/L3-8B-Stheno-v3.3-32K",
      "name": "Sao10K/L3-8B-Stheno-v3.3-32K",
      "provider": "huggingface_inference",
      "model_id": "Sao10K/L3-8B-Stheno-v3.3-32K"
    },
    {
      "id": "huggingface_inference/m42-health/Llama3-Med42-8B",
      "name": "m42-health/Llama3-Med42-8B",
      "provider": "huggingface_inference",
      "model_id": "m42-health/Llama3-Med42-8B"
    },
    {
      "id": "huggingface_inference/Snowflake/snowflake-arctic-embed-m-v1.5",
      "name": "Snowflake/snowflake-arctic-embed-m-v1.5",
      "provider": "huggingface_inference",
      "model_id": "Snowflake/snowflake-arctic-embed-m-v1.5"
    },
    {
      "id": "huggingface_inference/deepvk/USER-bge-m3",
      "name": "deepvk/USER-bge-m3",
      "provider": "huggingface_inference",
      "model_id": "deepvk/USER-bge-m3"
    },
    {
      "id": "huggingface_inference/TroyDoesAI/BlackSheep-X-Dolphin",
      "name": "TroyDoesAI/BlackSheep-X-Dolphin",
      "provider": "huggingface_inference",
      "model_id": "TroyDoesAI/BlackSheep-X-Dolphin"
    },
    {
      "id": "huggingface_inference/NousResearch/Meta-Llama-3.1-8B",
      "name": "NousResearch/Meta-Llama-3.1-8B",
      "provider": "huggingface_inference",
      "model_id": "NousResearch/Meta-Llama-3.1-8B",
      "benchmarks": {
        "arena_elo": 1211.0
      }
    },
    {
      "id": "huggingface_inference/NousResearch/Meta-Llama-3.1-8B-Instruct",
      "name": "NousResearch/Meta-Llama-3.1-8B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "NousResearch/Meta-Llama-3.1-8B-Instruct",
      "benchmarks": {
        "arena_elo": 1211.0
      }
    },
    {
      "id": "huggingface_inference/NousResearch/Meta-Llama-3.1-70B",
      "name": "NousResearch/Meta-Llama-3.1-70B",
      "provider": "huggingface_inference",
      "model_id": "NousResearch/Meta-Llama-3.1-70B",
      "benchmarks": {
        "arena_elo": 1293.0
      }
    },
    {
      "id": "huggingface_inference/NousResearch/Meta-Llama-3.1-70B-Instruct",
      "name": "NousResearch/Meta-Llama-3.1-70B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "NousResearch/Meta-Llama-3.1-70B-Instruct",
      "benchmarks": {
        "arena_elo": 1293.0
      }
    },
    {
      "id": "huggingface_inference/Orenguteng/Llama-3.1-8B-Lexi-Uncensored",
      "name": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored",
      "provider": "huggingface_inference",
      "model_id": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored"
    },
    {
      "id": "huggingface_inference/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2",
      "name": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2",
      "provider": "huggingface_inference",
      "model_id": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2"
    },
    {
      "id": "huggingface_inference/CohereLabs/c4ai-command-r-08-2024",
      "name": "CohereLabs/c4ai-command-r-08-2024",
      "provider": "huggingface_inference",
      "model_id": "CohereLabs/c4ai-command-r-08-2024",
      "benchmarks": {
        "mmlu": 0.652
      }
    },
    {
      "id": "huggingface_inference/MarinaraSpaghetti/NemoMix-Unleashed-12B",
      "name": "MarinaraSpaghetti/NemoMix-Unleashed-12B",
      "provider": "huggingface_inference",
      "model_id": "MarinaraSpaghetti/NemoMix-Unleashed-12B"
    },
    {
      "id": "huggingface_inference/HIT-TMG/KaLM-embedding-multilingual-mini-v1",
      "name": "HIT-TMG/KaLM-embedding-multilingual-mini-v1",
      "provider": "huggingface_inference",
      "model_id": "HIT-TMG/KaLM-embedding-multilingual-mini-v1"
    },
    {
      "id": "huggingface_inference/prithivMLmods/Canopus-Pixar-3D-Flux-LoRA",
      "name": "prithivMLmods/Canopus-Pixar-3D-Flux-LoRA",
      "provider": "huggingface_inference",
      "model_id": "prithivMLmods/Canopus-Pixar-3D-Flux-LoRA"
    },
    {
      "id": "huggingface_inference/mixedbread-ai/mxbai-embed-xsmall-v1",
      "name": "mixedbread-ai/mxbai-embed-xsmall-v1",
      "provider": "huggingface_inference",
      "model_id": "mixedbread-ai/mxbai-embed-xsmall-v1"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-7B",
      "name": "Qwen/Qwen2.5-7B",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-7B",
      "benchmarks": {
        "mmlu": 0.729
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-14B",
      "name": "Qwen/Qwen2.5-14B",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-14B",
      "benchmarks": {
        "mmlu": 0.799
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-72B",
      "name": "Qwen/Qwen2.5-72B",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-72B",
      "benchmarks": {
        "gpqa_diamond": 0.491,
        "mmlu": 0.834,
        "math_level_5": 0.632,
        "otis_mock_aime_2024_2025": 0.081,
        "trivia_qa": 0.719,
        "arena_elo": 1302.0
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-14B-Instruct",
      "name": "Qwen/Qwen2.5-14B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-14B-Instruct",
      "benchmarks": {
        "mmlu": 0.799
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-Coder-7B",
      "name": "Qwen/Qwen2.5-Coder-7B",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-Coder-7B",
      "benchmarks": {
        "mmlu": 0.68,
        "gsm8k": 0.839
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-32B-Instruct",
      "name": "Qwen/Qwen2.5-32B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-32B-Instruct",
      "benchmarks": {
        "gpqa_diamond": 0.461,
        "math_level_5": 0.561,
        "otis_mock_aime_2024_2025": 0.074
      }
    },
    {
      "id": "huggingface_inference/unsloth/Qwen2.5-7B-Instruct",
      "name": "unsloth/Qwen2.5-7B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "unsloth/Qwen2.5-7B-Instruct",
      "benchmarks": {
        "mmlu": 0.729
      }
    },
    {
      "id": "huggingface_inference/google/gemma-2-2b-jpn-it",
      "name": "google/gemma-2-2b-jpn-it",
      "provider": "huggingface_inference",
      "model_id": "google/gemma-2-2b-jpn-it"
    },
    {
      "id": "huggingface_inference/Orion-zhen/Qwen2.5-7B-Instruct-Uncensored",
      "name": "Orion-zhen/Qwen2.5-7B-Instruct-Uncensored",
      "provider": "huggingface_inference",
      "model_id": "Orion-zhen/Qwen2.5-7B-Instruct-Uncensored"
    },
    {
      "id": "huggingface_inference/UmeAiRT/FLUX.1-dev-LoRA-Modern_Pixel_art",
      "name": "UmeAiRT/FLUX.1-dev-LoRA-Modern_Pixel_art",
      "provider": "huggingface_inference",
      "model_id": "UmeAiRT/FLUX.1-dev-LoRA-Modern_Pixel_art"
    },
    {
      "id": "huggingface_inference/AlekseyCalvin/Alexander_BLOK_Flux_LoRA_SilverAgePoets_v1",
      "name": "AlekseyCalvin/Alexander_BLOK_Flux_LoRA_SilverAgePoets_v1",
      "provider": "huggingface_inference",
      "model_id": "AlekseyCalvin/Alexander_BLOK_Flux_LoRA_SilverAgePoets_v1"
    },
    {
      "id": "huggingface_inference/RLHFlow/LLaMA3.2-1B-SFT",
      "name": "RLHFlow/LLaMA3.2-1B-SFT",
      "provider": "huggingface_inference",
      "model_id": "RLHFlow/LLaMA3.2-1B-SFT"
    },
    {
      "id": "huggingface_inference/shibing624/chinese-text-correction-7b",
      "name": "shibing624/chinese-text-correction-7b",
      "provider": "huggingface_inference",
      "model_id": "shibing624/chinese-text-correction-7b"
    },
    {
      "id": "huggingface_inference/martintomov/ecom-flux-v2",
      "name": "martintomov/ecom-flux-v2",
      "provider": "huggingface_inference",
      "model_id": "martintomov/ecom-flux-v2"
    },
    {
      "id": "huggingface_inference/anthracite-org/magnum-v4-12b",
      "name": "anthracite-org/magnum-v4-12b",
      "provider": "huggingface_inference",
      "model_id": "anthracite-org/magnum-v4-12b"
    },
    {
      "id": "huggingface_inference/stabilityai/stable-diffusion-3.5-large-turbo",
      "name": "stabilityai/stable-diffusion-3.5-large-turbo",
      "provider": "huggingface_inference",
      "model_id": "stabilityai/stable-diffusion-3.5-large-turbo"
    },
    {
      "id": "huggingface_inference/genmo/mochi-1-preview",
      "name": "genmo/mochi-1-preview",
      "provider": "huggingface_inference",
      "model_id": "genmo/mochi-1-preview"
    },
    {
      "id": "huggingface_inference/CohereLabs/aya-expanse-32b",
      "name": "CohereLabs/aya-expanse-32b",
      "provider": "huggingface_inference",
      "model_id": "CohereLabs/aya-expanse-32b"
    },
    {
      "id": "huggingface_inference/Keltezaa/kirsten-dunst-actress-2000s-flux",
      "name": "Keltezaa/kirsten-dunst-actress-2000s-flux",
      "provider": "huggingface_inference",
      "model_id": "Keltezaa/kirsten-dunst-actress-2000s-flux"
    },
    {
      "id": "huggingface_inference/stabilityai/stable-diffusion-3.5-medium",
      "name": "stabilityai/stable-diffusion-3.5-medium",
      "provider": "huggingface_inference",
      "model_id": "stabilityai/stable-diffusion-3.5-medium"
    },
    {
      "id": "huggingface_inference/gokaygokay/Flux-Seamless-Texture-LoRA",
      "name": "gokaygokay/Flux-Seamless-Texture-LoRA",
      "provider": "huggingface_inference",
      "model_id": "gokaygokay/Flux-Seamless-Texture-LoRA"
    },
    {
      "id": "huggingface_inference/PygmalionAI/Pygmalion-3-12B",
      "name": "PygmalionAI/Pygmalion-3-12B",
      "provider": "huggingface_inference",
      "model_id": "PygmalionAI/Pygmalion-3-12B"
    },
    {
      "id": "huggingface_inference/gokaygokay/Flux-2D-Game-Assets-LoRA",
      "name": "gokaygokay/Flux-2D-Game-Assets-LoRA",
      "provider": "huggingface_inference",
      "model_id": "gokaygokay/Flux-2D-Game-Assets-LoRA"
    },
    {
      "id": "huggingface_inference/glif-loradex-trainer/bulbul_VST_Plugin",
      "name": "glif-loradex-trainer/bulbul_VST_Plugin",
      "provider": "huggingface_inference",
      "model_id": "glif-loradex-trainer/bulbul_VST_Plugin"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-Coder-14B-Instruct",
      "name": "Qwen/Qwen2.5-Coder-14B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-Coder-14B-Instruct",
      "benchmarks": {
        "mmlu": 0.752,
        "gsm8k": 0.887
      }
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-Coder-32B",
      "name": "Qwen/Qwen2.5-Coder-32B",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-Coder-32B",
      "benchmarks": {
        "mmlu": 0.791,
        "gsm8k": 0.911,
        "aider_polyglot": 8.0,
        "arena_elo": 1270.0
      }
    },
    {
      "id": "huggingface_inference/prithivMLmods/Retro-Pixel-Flux-LoRA",
      "name": "prithivMLmods/Retro-Pixel-Flux-LoRA",
      "provider": "huggingface_inference",
      "model_id": "prithivMLmods/Retro-Pixel-Flux-LoRA"
    },
    {
      "id": "huggingface_inference/prithivMLmods/Flux.1-Dev-LoRA-HDR-Realism",
      "name": "prithivMLmods/Flux.1-Dev-LoRA-HDR-Realism",
      "provider": "huggingface_inference",
      "model_id": "prithivMLmods/Flux.1-Dev-LoRA-HDR-Realism"
    },
    {
      "id": "huggingface_inference/FallenMerick/MN-Violet-Lotus-12B",
      "name": "FallenMerick/MN-Violet-Lotus-12B",
      "provider": "huggingface_inference",
      "model_id": "FallenMerick/MN-Violet-Lotus-12B"
    },
    {
      "id": "huggingface_inference/strangerzonehf/Flux-Ultimate-LoRA-Collection",
      "name": "strangerzonehf/Flux-Ultimate-LoRA-Collection",
      "provider": "huggingface_inference",
      "model_id": "strangerzonehf/Flux-Ultimate-LoRA-Collection"
    },
    {
      "id": "huggingface_inference/heydariAI/persian-embeddings",
      "name": "heydariAI/persian-embeddings",
      "provider": "huggingface_inference",
      "model_id": "heydariAI/persian-embeddings"
    },
    {
      "id": "huggingface_inference/dima806/fairface_age_image_detection",
      "name": "dima806/fairface_age_image_detection",
      "provider": "huggingface_inference",
      "model_id": "dima806/fairface_age_image_detection"
    },
    {
      "id": "huggingface_inference/scb10x/llama3.1-typhoon2-8b-instruct",
      "name": "scb10x/llama3.1-typhoon2-8b-instruct",
      "provider": "huggingface_inference",
      "model_id": "scb10x/llama3.1-typhoon2-8b-instruct"
    },
    {
      "id": "huggingface_inference/AlekseyCalvin/Sergey_Esenin_Poet_FluxLoRA_BySilverAgePoets",
      "name": "AlekseyCalvin/Sergey_Esenin_Poet_FluxLoRA_BySilverAgePoets",
      "provider": "huggingface_inference",
      "model_id": "AlekseyCalvin/Sergey_Esenin_Poet_FluxLoRA_BySilverAgePoets"
    },
    {
      "id": "huggingface_inference/Keltezaa/jennifer-garner-mid-2000s",
      "name": "Keltezaa/jennifer-garner-mid-2000s",
      "provider": "huggingface_inference",
      "model_id": "Keltezaa/jennifer-garner-mid-2000s"
    },
    {
      "id": "huggingface_inference/nlpai-lab/KURE-v1",
      "name": "nlpai-lab/KURE-v1",
      "provider": "huggingface_inference",
      "model_id": "nlpai-lab/KURE-v1"
    },
    {
      "id": "huggingface_inference/tasksource/ModernBERT-base-nli",
      "name": "tasksource/ModernBERT-base-nli",
      "provider": "huggingface_inference",
      "model_id": "tasksource/ModernBERT-base-nli"
    },
    {
      "id": "huggingface_inference/MoritzLaurer/ModernBERT-large-zeroshot-v2.0",
      "name": "MoritzLaurer/ModernBERT-large-zeroshot-v2.0",
      "provider": "huggingface_inference",
      "model_id": "MoritzLaurer/ModernBERT-large-zeroshot-v2.0"
    },
    {
      "id": "huggingface_inference/NAMAA-Space/masrawy-english-to-egyptian-arabic-translator-v2.9",
      "name": "NAMAA-Space/masrawy-english-to-egyptian-arabic-translator-v2.9",
      "provider": "huggingface_inference",
      "model_id": "NAMAA-Space/masrawy-english-to-egyptian-arabic-translator-v2.9"
    },
    {
      "id": "huggingface_inference/Sao10K/70B-L3.3-Cirrus-x1",
      "name": "Sao10K/70B-L3.3-Cirrus-x1",
      "provider": "huggingface_inference",
      "model_id": "Sao10K/70B-L3.3-Cirrus-x1"
    },
    {
      "id": "huggingface_inference/SakanaAI/TinySwallow-1.5B-Instruct",
      "name": "SakanaAI/TinySwallow-1.5B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "SakanaAI/TinySwallow-1.5B-Instruct"
    },
    {
      "id": "huggingface_inference/deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
      "name": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
      "provider": "huggingface_inference",
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
    },
    {
      "id": "huggingface_inference/deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
      "name": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
      "provider": "huggingface_inference",
      "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
      "benchmarks": {
        "gpqa_diamond": 0.557,
        "math_level_5": 0.899,
        "otis_mock_aime_2024_2025": 0.514
      }
    },
    {
      "id": "huggingface_inference/AdamLucek/ModernBERT-embed-base-legal-MRL",
      "name": "AdamLucek/ModernBERT-embed-base-legal-MRL",
      "provider": "huggingface_inference",
      "model_id": "AdamLucek/ModernBERT-embed-base-legal-MRL"
    },
    {
      "id": "huggingface_inference/Alpha-VLLM/Lumina-Image-2.0",
      "name": "Alpha-VLLM/Lumina-Image-2.0",
      "provider": "huggingface_inference",
      "model_id": "Alpha-VLLM/Lumina-Image-2.0"
    },
    {
      "id": "huggingface_inference/SentientAGI/Dobby-Mini-Unhinged-Llama-3.1-8B",
      "name": "SentientAGI/Dobby-Mini-Unhinged-Llama-3.1-8B",
      "provider": "huggingface_inference",
      "model_id": "SentientAGI/Dobby-Mini-Unhinged-Llama-3.1-8B"
    },
    {
      "id": "huggingface_inference/mistralai/Mistral-Small-24B-Instruct-2501",
      "name": "mistralai/Mistral-Small-24B-Instruct-2501",
      "provider": "huggingface_inference",
      "model_id": "mistralai/Mistral-Small-24B-Instruct-2501",
      "benchmarks": {
        "arena_elo": 1274.0
      }
    },
    {
      "id": "huggingface_inference/AymanTarig/Llama-3.2-1B-FC-v3",
      "name": "AymanTarig/Llama-3.2-1B-FC-v3",
      "provider": "huggingface_inference",
      "model_id": "AymanTarig/Llama-3.2-1B-FC-v3"
    },
    {
      "id": "huggingface_inference/prithivMLmods/Deep-Fake-Detector-v2-Model",
      "name": "prithivMLmods/Deep-Fake-Detector-v2-Model",
      "provider": "huggingface_inference",
      "model_id": "prithivMLmods/Deep-Fake-Detector-v2-Model"
    },
    {
      "id": "huggingface_inference/AdithyaSK/Qwen-0.5b-Code-Reasoning",
      "name": "AdithyaSK/Qwen-0.5b-Code-Reasoning",
      "provider": "huggingface_inference",
      "model_id": "AdithyaSK/Qwen-0.5b-Code-Reasoning"
    },
    {
      "id": "huggingface_inference/TheFinAI/Fino1-8B",
      "name": "TheFinAI/Fino1-8B",
      "provider": "huggingface_inference",
      "model_id": "TheFinAI/Fino1-8B"
    },
    {
      "id": "huggingface_inference/trendmicro-ailab/Llama-Primus-Reasoning",
      "name": "trendmicro-ailab/Llama-Primus-Reasoning",
      "provider": "huggingface_inference",
      "model_id": "trendmicro-ailab/Llama-Primus-Reasoning"
    },
    {
      "id": "huggingface_inference/ByteDance-Seed/BFS-Prover-V1-7B",
      "name": "ByteDance-Seed/BFS-Prover-V1-7B",
      "provider": "huggingface_inference",
      "model_id": "ByteDance-Seed/BFS-Prover-V1-7B"
    },
    {
      "id": "huggingface_inference/qihoo360/TinyR1-32B-Preview",
      "name": "qihoo360/TinyR1-32B-Preview",
      "provider": "huggingface_inference",
      "model_id": "qihoo360/TinyR1-32B-Preview"
    },
    {
      "id": "huggingface_inference/Wan-AI/Wan2.1-T2V-14B",
      "name": "Wan-AI/Wan2.1-T2V-14B",
      "provider": "huggingface_inference",
      "model_id": "Wan-AI/Wan2.1-T2V-14B"
    },
    {
      "id": "huggingface_inference/deepvk/USER2-base",
      "name": "deepvk/USER2-base",
      "provider": "huggingface_inference",
      "model_id": "deepvk/USER2-base"
    },
    {
      "id": "huggingface_inference/CohereLabs/aya-vision-32b",
      "name": "CohereLabs/aya-vision-32b",
      "provider": "huggingface_inference",
      "model_id": "CohereLabs/aya-vision-32b"
    },
    {
      "id": "huggingface_inference/OpenPipe/Deductive-Reasoning-Qwen-32B",
      "name": "OpenPipe/Deductive-Reasoning-Qwen-32B",
      "provider": "huggingface_inference",
      "model_id": "OpenPipe/Deductive-Reasoning-Qwen-32B"
    },
    {
      "id": "huggingface_inference/sergeyzh/BERTA",
      "name": "sergeyzh/BERTA",
      "provider": "huggingface_inference",
      "model_id": "sergeyzh/BERTA"
    },
    {
      "id": "huggingface_inference/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
      "name": "mistralai/Mistral-Small-3.1-24B-Instruct-2503",
      "provider": "huggingface_inference",
      "model_id": "mistralai/Mistral-Small-3.1-24B-Instruct-2503",
      "benchmarks": {
        "arena_elo": 1304.0
      }
    },
    {
      "id": "huggingface_inference/prithivMLmods/Dog-Breed-120",
      "name": "prithivMLmods/Dog-Breed-120",
      "provider": "huggingface_inference",
      "model_id": "prithivMLmods/Dog-Breed-120"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen2.5-VL-32B-Instruct",
      "name": "Qwen/Qwen2.5-VL-32B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen2.5-VL-32B-Instruct"
    },
    {
      "id": "huggingface_inference/ToastyPigeon/Gemma-3-Starshine-12B",
      "name": "ToastyPigeon/Gemma-3-Starshine-12B",
      "provider": "huggingface_inference",
      "model_id": "ToastyPigeon/Gemma-3-Starshine-12B"
    },
    {
      "id": "huggingface_inference/CrabInHoney/urlbert-tiny-v4-malicious-url-classifier",
      "name": "CrabInHoney/urlbert-tiny-v4-malicious-url-classifier",
      "provider": "huggingface_inference",
      "model_id": "CrabInHoney/urlbert-tiny-v4-malicious-url-classifier"
    },
    {
      "id": "huggingface_inference/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
      "name": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
      "benchmarks": {
        "gpqa_diamond": 0.67,
        "math_level_5": 0.73,
        "otis_mock_aime_2024_2025": 0.206,
        "frontiermath": 0.007,
        "aider_polyglot": 15.6,
        "simplebench": 0.277
      }
    },
    {
      "id": "huggingface_inference/HiDream-ai/HiDream-I1-Full",
      "name": "HiDream-ai/HiDream-I1-Full",
      "provider": "huggingface_inference",
      "model_id": "HiDream-ai/HiDream-I1-Full"
    },
    {
      "id": "huggingface_inference/facebook/KernelLLM",
      "name": "facebook/KernelLLM",
      "provider": "huggingface_inference",
      "model_id": "facebook/KernelLLM"
    },
    {
      "id": "huggingface_inference/nvidia/OpenCodeReasoning-Nemotron-7B",
      "name": "nvidia/OpenCodeReasoning-Nemotron-7B",
      "provider": "huggingface_inference",
      "model_id": "nvidia/OpenCodeReasoning-Nemotron-7B"
    },
    {
      "id": "huggingface_inference/lokeshch19/ModernPubMedBERT",
      "name": "lokeshch19/ModernPubMedBERT",
      "provider": "huggingface_inference",
      "model_id": "lokeshch19/ModernPubMedBERT"
    },
    {
      "id": "huggingface_inference/meta-llama/Llama-Guard-4-12B",
      "name": "meta-llama/Llama-Guard-4-12B",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Llama-Guard-4-12B"
    },
    {
      "id": "huggingface_inference/allura-org/GLM4-9B-Neon-v2",
      "name": "allura-org/GLM4-9B-Neon-v2",
      "provider": "huggingface_inference",
      "model_id": "allura-org/GLM4-9B-Neon-v2"
    },
    {
      "id": "huggingface_inference/fdtn-ai/Foundation-Sec-8B",
      "name": "fdtn-ai/Foundation-Sec-8B",
      "provider": "huggingface_inference",
      "model_id": "fdtn-ai/Foundation-Sec-8B"
    },
    {
      "id": "huggingface_inference/unsloth/Qwen3-8B",
      "name": "unsloth/Qwen3-8B",
      "provider": "huggingface_inference",
      "model_id": "unsloth/Qwen3-8B"
    },
    {
      "id": "huggingface_inference/Qwen/Qwen3-8B-Base",
      "name": "Qwen/Qwen3-8B-Base",
      "provider": "huggingface_inference",
      "model_id": "Qwen/Qwen3-8B-Base"
    },
    {
      "id": "huggingface_inference/XGenerationLab/XiYanSQL-QwenCoder-7B-2504",
      "name": "XGenerationLab/XiYanSQL-QwenCoder-7B-2504",
      "provider": "huggingface_inference",
      "model_id": "XGenerationLab/XiYanSQL-QwenCoder-7B-2504"
    },
    {
      "id": "huggingface_inference/meta-llama/Llama-Prompt-Guard-2-86M",
      "name": "meta-llama/Llama-Prompt-Guard-2-86M",
      "provider": "huggingface_inference",
      "model_id": "meta-llama/Llama-Prompt-Guard-2-86M"
    },
    {
      "id": "huggingface_inference/Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
      "name": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1",
      "provider": "huggingface_inference",
      "model_id": "Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1"
    },
    {
      "id": "huggingface_inference/mlabonne/Qwen3-14B-abliterated",
      "name": "mlabonne/Qwen3-14B-abliterated",
      "provider": "huggingface_inference",
      "model_id": "mlabonne/Qwen3-14B-abliterated"
    },
    {
      "id": "huggingface_inference/SWE-bench/SWE-agent-LM-32B",
      "name": "SWE-bench/SWE-agent-LM-32B",
      "provider": "huggingface_inference",
      "model_id": "SWE-bench/SWE-agent-LM-32B"
    },
    {
      "id": "huggingface_inference/allura-org/remnant-glm4-32b",
      "name": "allura-org/remnant-glm4-32b",
      "provider": "huggingface_inference",
      "model_id": "allura-org/remnant-glm4-32b"
    },
    {
      "id": "huggingface_inference/zone659/NUDEWOMEN",
      "name": "zone659/NUDEWOMEN",
      "provider": "huggingface_inference",
      "model_id": "zone659/NUDEWOMEN"
    },
    {
      "id": "huggingface_inference/PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
      "name": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b",
      "provider": "huggingface_inference",
      "model_id": "PocketDoc/Dans-PersonalityEngine-V1.3.0-24b"
    },
    {
      "id": "huggingface_inference/a-m-team/AM-Thinking-v1",
      "name": "a-m-team/AM-Thinking-v1",
      "provider": "huggingface_inference",
      "model_id": "a-m-team/AM-Thinking-v1"
    },
    {
      "id": "huggingface_inference/Intelligent-Internet/II-Medical-8B",
      "name": "Intelligent-Internet/II-Medical-8B",
      "provider": "huggingface_inference",
      "model_id": "Intelligent-Internet/II-Medical-8B"
    },
    {
      "id": "huggingface_inference/rd211/Qwen3-1.7B-Instruct",
      "name": "rd211/Qwen3-1.7B-Instruct",
      "provider": "huggingface_inference",
      "model_id": "rd211/Qwen3-1.7B-Instruct"
    },
    {
      "id": "huggingface_inference/ArliAI/QwQ-32B-ArliAI-RpR-v4",
      "name": "ArliAI/QwQ-32B-ArliAI-RpR-v4",
      "provider": "huggingface_inference",
      "model_id": "ArliAI/QwQ-32B-ArliAI-RpR-v4"
    },
    {
      "id": "huggingface_inference/deepseek-ai/DeepSeek-R1-0528",
      "name": "deepseek-ai/DeepSeek-R1-0528",
      "provider": "huggingface_inference",
      "model_id": "deepseek-ai/DeepSeek-R1-0528",
      "benchmarks": {
        "gpqa_diamond": 0.692,
        "math_level_5": 0.931,
        "otis_mock_aime_2024_2025": 0.533,
        "aider_polyglot": 56.9,
        "simplebench": 0.309,
        "simpleqa_verified": 0.274,
        "swe_bench_verified": 0.333,
        "arena_elo": 1396.0
      }
    },
    {
      "id": "huggingface_inference/Seanwang1221/Yangmi_SD15_FLUX",
      "name": "Seanwang1221/Yangmi_SD15_FLUX",
      "provider": "huggingface_inference",
      "model_id": "Seanwang1221/Yangmi_SD15_FLUX"
    },
    {
      "id": "huggingface_inference/Seanwang1221/FanBingbing_FLUX",
      "name": "Seanwang1221/FanBingbing_FLUX",
      "provider": "huggingface_inference",
      "model_id": "Seanwang1221/FanBingbing_FLUX"
    },
    {
      "id": "huggingface_inference/Menlo/Jan-nano",
      "name": "Menlo/Jan-nano",
      "provider": "huggingface_inference",
      "model_id": "Menlo/Jan-nano"
    },
    {
      "id": "huggingface_inference/uomene/dafuq",
      "name": "uomene/dafuq",
      "provider": "huggingface_inference",
      "model_id": "uomene/dafuq"
    },
    {
      "id": "huggingface_inference/allura-org/Q3-8B-Kintsugi",
      "name": "allura-org/Q3-8B-Kintsugi",
      "provider": "huggingface_inference",
      "model_id": "allura-org/Q3-8B-Kintsugi"
    },
    {
      "id": "huggingface_inference/Intelligent-Internet/II-Medical-8B-1706",
      "name": "Intelligent-Internet/II-Medical-8B-1706",
      "provider": "huggingface_inference",
      "model_id": "Intelligent-Internet/II-Medical-8B-1706"
    },
    {
      "id": "huggingface_inference/netease-youdao/Confucius3-Math",
      "name": "netease-youdao/Confucius3-Math",
      "provider": "huggingface_inference",
      "model_id": "netease-youdao/Confucius3-Math"
    },
    {
      "id": "huggingface_inference/baidu/ERNIE-4.5-0.3B-PT",
      "name": "baidu/ERNIE-4.5-0.3B-PT",
      "provider": "huggingface_inference",
      "model_id": "baidu/ERNIE-4.5-0.3B-PT"
    },
    {
      "id": "huggingface_inference/baidu/ERNIE-4.5-21B-A3B-PT",
      "name": "baidu/ERNIE-4.5-21B-A3B-PT",
      "provider": "huggingface_inference",
      "model_id": "baidu/ERNIE-4.5-21B-A3B-PT"
    }
  ]
}